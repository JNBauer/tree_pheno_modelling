[["index.html", "Tree phenology analysis with R - Logbook Chapter 1 Introduction", " Tree phenology analysis with R - Logbook Julian Bauer Chapter 1 Introduction I can still remember our first lesson, where the question went around how our state of R knowledge is. At that time I already had contact with R several times and also worked with it a lot in the Decision Analysis course, but I still felt like a beginner. Now at the end of this module, after learning so many new things, I don’t feel that way anymore. Because of the module, I can now not only transform dataframes to my own needs in various ways, but I have also learned about many new packages and was introduced to the world of dormancy. I am very happy that the module is offered despite the low survivor rate and I think that it could help many people to be much more secure in dealing with R. Thank you very much! "],["packages.html", "Chapter 2 Packages", " Chapter 2 Packages 2.0.1 Bookdown I chose the bookdown approach for a better organization. The chapters are always in single .rmd files, which makes knitting much faster, if I just want to have a look at the chapter I am currently working on. Sometimes there are problems with packages that have to be reloaded, but I am glad to have transformed the book into the Bookdown format quite early. The great documentation made it possible for me to include many special features. 2.0.2 chillR The key package of this logbook contains all important tools to perform phenology analysis. It allows researchers to work flexibly with different agroclimatic metrics. Previous hurdles led to limitations in the choice of chill models, which are overcome with ChillR. The reference CRAN website and the course website provide useful instructions on how to work with the package. 2.0.3 dplyr family Packages of the tidyverse universe like dplyr or purr can contain very useful functions, which could be solved with base R ocde often only more awkwardly. Whether this means that all data processing steps have to be done with functions from these packages, is something I don’t know. In the end it is important to know that there are always several ways to reach the goal. 2.0.4 ggplot2 ggplot2 is essential for visualizing the data. Achieving the same results with Base R functions is more complicated and often does not visually match the results of ggplot2. Websites like the R graph gallery help to realize own ideas for graphs. 2.0.5 leaflet The leaflet packages purpose is mainly to create interactive maps for HTML output. The possibilities to display various spatial data are enormous, but in this logbook I only present relatively simple applications. 2.0.6 kableExtra The kableExtra package enables the creation of tables. The customization of the formatting and appearance of the data makes it an important part of the logbook. There are also other packages for making tables like DT which I unfortunately could not try out in this module, but which might be even more suitable for HTML. 2.0.7 plotly This package allows the creation of interactive graphs, which can be easily used as part of HTML documents. "],["tree-dormancy.html", "Chapter 3 Tree dormancy 3.1 Put yourself in the place of a breeder who wants to calculate the temperature requirements of a newly released cultivar. Which method will you use to calculate the chilling and forcing periods? Please justify your answer. 3.2 Which are the advantages (2) of the BBCH scale compared with earlies scales? 3.3 Classify the following phenological stages of sweet cherry according to the BBCH scale:", " Chapter 3 Tree dormancy 3.1 Put yourself in the place of a breeder who wants to calculate the temperature requirements of a newly released cultivar. Which method will you use to calculate the chilling and forcing periods? Please justify your answer. At the moment the Dynamic model is the most sophisticated approach, because of its process based idea. This leads on the other hand to more complicated calculations compared to prior methods . Models like the chilling hours are easier to calculate, but do not longer meet the needed requirements. I would recommend the breeder to use the Dynamic model, because of its high reliability. The temperature as only input is the same for all models. 3.2 Which are the advantages (2) of the BBCH scale compared with earlies scales? The BBCH scale covers the whole development of the plant not only the buds as with earlier scales. The second advantage is that the BBCH scale is developed in a way that it can be adapted to all plant species. Additionally the numerical structured order is more comprehensive. 3.3 Classify the following phenological stages of sweet cherry according to the BBCH scale: Figure 3.1: Phenological stages of sweet cherry buds 54/55 65 could be also lower if the object in the background is a (closed) bud 89 "],["climate-change-and-impact-projection.html", "Chapter 4 Climate change and impact projection 4.1 List the main drivers of climate change at the decade to century scale, and briefly explain the mechanism through which the currently most important driver affects our climate. 4.2 Explain briefly what is special about temperature dynamics of the recent decades, and why we have good reasons to be concerned. 4.3 What does the abbreviation ‘RCP’ stand for, how are RCPs defined, and what is their role in projecting future climates? 4.4 Briefly describe the 3 climate impact projection methods described in the fourth video.", " Chapter 4 Climate change and impact projection As introduction to the topic, I created a Warming Stripes plot with ggplot2, which is based on the geom_tile() function. For the temperature data, I used the record from CKA. With an even longer data set, the rapid change in climate would be even more apparent, although it is already very impressively visible with this short data set. library(chillR) library(dplyr) library(RColorBrewer) library(ggplot2) col_strip &lt;- brewer.pal(11, &quot;RdBu&quot;) weather&lt;-read_tab(&quot;data/TMaxTMin1958-2019_patched.csv&quot;) %&gt;% mutate(Tmean = (Tmin + Tmax)/2) plot &lt;- aggregate(weather$Tmean, list(weather$Year), FUN=mean) %&gt;% mutate(height = 1) %&gt;% rename(&quot;Tmean&quot;=&quot;x&quot;) %&gt;% rename(&quot;year&quot;=&quot;Group.1&quot;) %&gt;% ggplot(aes(x = year, y = 1, fill = Tmean)) + geom_tile() + scale_fill_gradientn(colors = rev(col_strip), name = &quot;Yearly mean\\nTemperature °C&quot;) + theme_void() Show/Hide code Figure 4.1: Warming stripes plotted for data from CKA between 1958 and 2019 4.1 List the main drivers of climate change at the decade to century scale, and briefly explain the mechanism through which the currently most important driver affects our climate. Greenhouse gases (Decade) Aerosols (Decade) Surface albedo Clouds Ozone Volcanic and meteorite activity (Centuries) Ocean currents (Centuries) Milankovic cycles (Centuries) Sun (Centuries) The most important driver at the moment are Greenhouse gases (GHGs) and in particular anthropogenic CO2, which is mainly released by burning fossil fuels or by the production of cement. The concentration of CO2 in the atmosphere, which was closely correlated with the temperature in the past, is rising steadily and steeply at the moment. GHGs only absorb the reflected radiation coming from the earth and are not affected by direct radiation from the sun. This effect works like a trap, where radiation can enter, but not leave the atmosphere, which leads to an increase in temperature. 4.2 Explain briefly what is special about temperature dynamics of the recent decades, and why we have good reasons to be concerned. In the last decades, the temperature rose steadily. The last two decades were the warmest measured since 1880. We are about to live in a world, that has never been so warm for humans or humanlike beings. The climate of the earth was always changing, but the speed of change was never so fast before. When at some point a tipping point is reached the changes cannot be undone. 4.3 What does the abbreviation ‘RCP’ stand for, how are RCPs defined, and what is their role in projecting future climates? RCP is the abbreviation for Representative Concentration Pathways, which are defined by the additional radiated forcing in watts per square meter for the year 2100. They serve as scenarios for the future of our climate. A high scenario like RCP8.5 is caused by increasing GHG emissions, while lower scenarios can be achieved with a decrease of GHG emissions. In the case of RCP6.0, additional 6 W m-2 are expected for the year 2100. Multiple scenarios should be considered, since one does not know which RCP prediction will occur. 4.4 Briefly describe the 3 climate impact projection methods described in the fourth video. 4.4.1 Statistical models Statistical models are created from the relationship between two or more variables, from which statements about future conditions are then to be made. Typical statistical model are based on regression for example. Statistical models do not focus on the process behind the data. 4.4.2 Process-based models Process-based models try to represent and link the essential processes of a system. In this way, the knowledge of current research can also flow into the results. The creation of such models is often more complicated in comparison to statistical models, since the represented environments are often very complex. It is important to find the right balance that makes a model robust without neglecting significant precise processes. 4.4.3 Climate analogue models Climate analogue models orientate on places that experience already different climate conditions. If we expect warmer conditions in the future for our selected location, we can search for regions that already have these conditions in the present. There are often more factors influencing the systems beside the climate, which are usually not considered by climate analogue models. Such climate analogue models can often be applied in a south north gradient, but also on altitude differences. "],["winter-chill-projections.html", "Chapter 5 Winter chill projections 5.1 Sketch out three data access and processing challenges that had to be overcome in order to produce chill projections with state-of-the-art methodology. 5.2 Outline, in your understanding, the basic steps that are necessary to make such projections.", " Chapter 5 Winter chill projections 5.1 Sketch out three data access and processing challenges that had to be overcome in order to produce chill projections with state-of-the-art methodology. Climate data can easily accumulate to high amounts of computer data, which can make further processing highly demanding in time and computing. But good data management and increasingly efficient computers enable tasks like this. Over time, various climate models were created and replaced with improved ones, after new insights were gathered. The changes from SRESs to SSPs and later to RCPs increased the quality of the forecasts, but for users of those forecasts it meant adapting to the new standard. To achieve state-of-the-art chill projections, so called ensembles are used. These consist of several chilling models plotted together. The idea is, that there is no single correct model, but multiple models that represent the chill projections. 5.2 Outline, in your understanding, the basic steps that are necessary to make such projections. setting the spatial scope for the projections setting the temporal scope for the projections choosing the ensemble of chill models accessing the data from the past prepare the data generating the data for the future climate with a weather generator apply the ensemble of chill models plot the results "],["manual-chill-analysis.html", "Chapter 6 Manual chill analysis 6.1 Write a basic function that calculates warm hours (&gt;25°C) 6.2 Apply this function to the Winters_hours_gaps dataset 6.3 Extend this function, so that it can take start and end dates as inputs and sums up warm hours between these dates", " Chapter 6 Manual chill analysis 6.1 Write a basic function that calculates warm hours (&gt;25°C) The Winters_hours_gaps dataset is part of the ChillR package. It was recorded 2008 in a walnut orchard in Winters, California. The dataset was modified by excluding a not used Temp_gaps column. Table 6.1: Table for the modified Winters_hours_gaps dataset Year Month Day Hour Temp 2008 3 3 10 15.127 2008 3 3 11 17.153 2008 3 3 12 18.699 2008 3 3 13 18.699 2008 3 3 14 18.842 The function creates a new column Warming_hours that contains the result of the comparison if hourtemps$Tempis greater than 25. WH &lt;- function(hourtemps) { hourtemps[,&quot;Warming_hours&quot;] &lt;- hourtemps$Temp&gt;25 return(hourtemps) } 6.2 Apply this function to the Winters_hours_gaps dataset Winters_hours_gaps_plus_WH &lt;- WH(Winters_hours_gaps) sum_whgpWH &lt;- sum(Winters_hours_gaps_plus_WH$Warming_hours) When applying the function to the Winter_hours_gaps dataset and taking the sum of the Winters_hours_gaps_plus_WH$Warming_hours coloumn, 1275 hours over 25°C occured. In a table, it could be displayed like this: Table 6.2: Table for Winters_hours_gaps_plus_WH Year Month Day Hour Temp_gaps Temp 2008 8 17 5 15.796 15.796 2008 8 17 6 16.225 16.225 2008 8 17 7 17.510 17.51 2008 8 17 8 19.674 19.674 2008 8 17 9 22.082 22.082 2008 8 17 10 24.508 24.508 2008 8 17 11 25.404 25.404 2008 8 17 12 26.842 26.842 2008 8 17 13 28.171 28.171 2008 8 17 14 27.949 27.949 6.3 Extend this function, so that it can take start and end dates as inputs and sums up warm hours between these dates The following function contains additional input variables for the start and end dates. The variables are used to index the correct rows between which the sum of the warm hours should be taken. The previously created WH function is used to compare the temperature to the threshold. WH_YEARMODA &lt;- function(hourtemps, start_dateYEARMODA,end_dateYEARMODA, start_hour=12, end_hour=12) { start_year &lt;- as.numeric(substr(start_dateYEARMODA, 1, 4)) start_month &lt;- as.numeric(substr(start_dateYEARMODA, 5, 6)) start_day &lt;- as.numeric(substr(start_dateYEARMODA, 7, 8)) end_year &lt;- as.numeric(substr(end_dateYEARMODA, 1, 4)) end_month &lt;- as.numeric(substr(end_dateYEARMODA, 5, 6)) end_day &lt;- as.numeric(substr(end_dateYEARMODA, 7, 8)) start_date &lt;- which(hourtemps$Year==start_year &amp; hourtemps$Month==start_month &amp; hourtemps$Day==start_day &amp; hourtemps$Hour==start_hour) end_date &lt;- which(hourtemps$Year==end_year &amp; hourtemps$Month==end_month &amp; hourtemps$Day==end_day &amp; hourtemps$Hour==end_hour) warm_hours &lt;- WH(hourtemps) WHs &lt;- sum(warm_hours[start_date:end_date, &quot;Warming_hours&quot;]) return(WHs) } warming_hours_for_time_frame &lt;- WH_YEARMODA(hourtemps, 20080501, 20080831) warming_hours_for_time_frame ## [1] 957 Between the 1th of May 2008 (20080501) and the 31th of August 2008 (20080831), 957 hours over 25°C occurred. "],["chill-models.html", "Chapter 7 Chill Models 7.1 Run the chilling() function on the Winters_hours_gap dataset 7.2 Create your own temperature-weighting chill model using the step_model() function. 7.3 Run this model on the Winters_hours_gaps dataset using the tempResponse() function.", " Chapter 7 Chill Models 7.1 Run the chilling() function on the Winters_hours_gap dataset The chilling function calculates different chilling methods and GDH. require(chillR) chilling_output &lt;- chilling(make_JDay(Winters_hours_gaps),Start_JDay = 80, End_JDay = 120) Table 7.1: Chilling output for a part of the Winters_hours_gap Season End_year Season_days Data_days Perc_complete Chilling_Hours Utah_Model Chill_portions GDH 2007/2008 2008 41 41 100 106 -57.5 4.191134 9682.99 7.2 Create your own temperature-weighting chill model using the step_model() function. own_utah_steps &lt;- data.frame( lower=c(-291,0,1,2,3,4,5,6), upper=c(0,1,2,3,4,5,6,100), weight=c(0,0.5,1,2,3,2,1,0)) Table 7.2: Table for own_utah_steps lower upper weight -291 0 0.0 0 1 0.5 1 2 1.0 2 3 2.0 3 4 3.0 4 5 2.0 5 6 1.0 6 100 0.0 Testing the modified own_utah_steps alone. own_utah &lt;- function(x) step_model(x,own_utah_steps) test &lt;- own_utah(Winters_hours_gaps$Temp) head(test) ## [1] 0 0 0 0 0 0 7.3 Run this model on the Winters_hours_gaps dataset using the tempResponse() function. In the next code chunk, the Modified_Utah_Model is included in the list of the tempResponse() function. output&lt;-tempResponse(make_JDay(Winters_hours_gaps), Start_JDay = 80, End_JDay = 100, models=list(Modified_Utah_Model=own_utah,Chill_portions=Dynamic_Model), whole_record = FALSE) Table 7.3: TempResponse output for the chill of the Modified_Utah_Model and Chill_portions Season End_year Season_days Data_days Perc_complete Modified_Utah_Model Chill_portions 2007/2008 2008 21 21 100 52 2.942197 "],["making-hourly-temperatures.html", "Chapter 8 Making hourly temperatures 8.1 Choose a location of interest, find out its latitude and produce plots of daily sunrise, sunset and daylength 8.2 Produce an hourly dataset, based on idealized daily curves, for the KA_weather dataset (included in chillR) 8.3 Produce empirical temperature curve parameters for the Winters_hours_gaps dataset, and use them to predict hourly values from daily temperatures (this is very similar to the example above, but please make sure you understand what’s going on)", " Chapter 8 Making hourly temperatures 8.1 Choose a location of interest, find out its latitude and produce plots of daily sunrise, sunset and daylength The chosen location with the latitude of 32 °North (`lat = 32``) can be seen below in the leaflet. It is a farm located in the south of Morocco. Figure 8.1: Leaflet map for the chosen location in Morocco The chillR daylength function can be used to get the data for the sunsets and sunrises for a specified latitude. The melt command of the reshape2 package is used to bring the dataset in the “long” format, that is more suitable for the use with ggplot2. require(chillR) require(reshape2) day_infos&lt;-daylength(latitude=32,JDay=1:365) days_df&lt;-data.frame(JDay=1:365, Sunrise=day_infos$Sunrise, Sunset=day_infos$Sunset, Daylength=day_infos$Daylength) days_df&lt;-melt(days_df, id=c(&quot;JDay&quot;)) The days_df data is plotted in 3 line graphs (geom_line) split by the factor variable. require(ggplot2) ggplot(days_df, aes(JDay, value)) + geom_line(lwd=1.5) + facet_grid(cols=vars(variable)) + ylab(&quot;Time of Day / Daylength (Hours)&quot;) + theme_bw(base_size = 20) Figure 8.2: Daylength for a location with latitude of 32 °North (Morocco) 8.2 Produce an hourly dataset, based on idealized daily curves, for the KA_weather dataset (included in chillR) The stack_hourly_temps function is able to simulate idealized hourly data with daily minimal temperatures, daily maximal temperatures and the latitude of the place of interest. hourly_temps_KA &lt;- stack_hourly_temps(KA_weather, latitude=50.4) #str(hourly_temps_KA) # to get the correct item from the list hourly_temps_KA_df &lt;- hourly_temps_KA$hourtemps hourly_temps_KA_df[,&quot;connected_timedate&quot;] &lt;- ISOdate( year = hourly_temps_KA_df$Year, month = hourly_temps_KA_df$Month, day = hourly_temps_KA_df$Day, hour = hourly_temps_KA_df$Hour) ggplot(hourly_temps_KA_df[3500:3660,], aes(connected_timedate,Temp)) + geom_line(lwd=1.5) + labs(x=&quot;Date&quot;, y=&quot;Temperature in C°&quot;, title = &quot;Hourly modelled temperature for Week 22 in 2002&quot;) + theme_bw(base_size = 14) Figure 8.3: Hourly modelled temperature for Week 22 in 2002 8.3 Produce empirical temperature curve parameters for the Winters_hours_gaps dataset, and use them to predict hourly values from daily temperatures (this is very similar to the example above, but please make sure you understand what’s going on) The Empirical_daily_temperature_curve function is used to extract hourly coefficients for each month from the Winters_hours_gaps. The make_all_day_table function is used to calculate Tmin and Tmax values, that are used in combination with the coefficients in the Empirical_hourly_temperatures to calculate the hourly temperatures. coeffs&lt;-Empirical_daily_temperature_curve(Winters_hours_gaps) winters_daily&lt;-make_all_day_table(Winters_hours_gaps, input_timestep=&quot;hour&quot;) winters_empirical&lt;-Empirical_hourly_temperatures(winters_daily,coeffs) Table 8.1: Modified Table for empirical hourly temperatures DATE Tmin Tmax JDay Hour Temp 1 2008-03-03 12:00:00 7.92 19.508 63 0 9.976706 41 2008-03-03 12:00:00 7.92 19.508 63 1 9.716943 69 2008-03-03 12:00:00 7.92 19.508 63 2 9.409812 91 2008-03-03 12:00:00 7.92 19.508 63 3 9.247518 133 2008-03-03 12:00:00 7.92 19.508 63 4 8.726599 151 2008-03-03 12:00:00 7.92 19.508 63 5 8.313508 183 2008-03-03 12:00:00 7.92 19.508 63 6 7.920000 207 2008-03-03 12:00:00 7.92 19.508 63 7 8.282838 235 2008-03-03 12:00:00 7.92 19.508 63 8 11.537726 273 2008-03-03 12:00:00 7.92 19.508 63 9 13.659805 Hourly data with the Empirical and ideal method are compared with the real measured temperature. The melt function is again used to bring the dataset into the right shape. require(reshape2) winters_empirical&lt;-winters_empirical[,c(&quot;Year&quot;,&quot;Month&quot;,&quot;Day&quot;,&quot;Hour&quot;,&quot;Temp&quot;)] colnames(winters_empirical)[ncol(winters_empirical)]&lt;-&quot;Temp_empirical&quot; winters_ideal&lt;-stack_hourly_temps(winters_daily, latitude=38.5)$hourtemps winters_ideal&lt;-winters_ideal[,c(&quot;Year&quot;,&quot;Month&quot;,&quot;Day&quot;,&quot;Hour&quot;,&quot;Temp&quot;)] colnames(winters_ideal)[ncol(winters_ideal)]&lt;-&quot;Temp_ideal&quot; winters_temps&lt;-merge(Winters_hours_gaps,winters_empirical, by=c(&quot;Year&quot;,&quot;Month&quot;,&quot;Day&quot;,&quot;Hour&quot;)) winters_temps&lt;-merge(winters_temps,winters_ideal, by=c(&quot;Year&quot;,&quot;Month&quot;,&quot;Day&quot;,&quot;Hour&quot;)) winters_temps[,&quot;DATE&quot;]&lt;-ISOdate(winters_temps$Year, winters_temps$Month, winters_temps$Day, winters_temps$Hour) winters_temps_to_plot&lt;-winters_temps[,c(&quot;DATE&quot;, &quot;Temp&quot;, &quot;Temp_empirical&quot;, &quot;Temp_ideal&quot;)] winters_temps_to_plot&lt;-winters_temps_to_plot[200:262,] winters_temps_to_plot&lt;-melt(winters_temps_to_plot, id=c(&quot;DATE&quot;)) colnames(winters_temps_to_plot)&lt;-c(&quot;DATE&quot;,&quot;Method&quot;,&quot;Temperature&quot;) ggplot(data=winters_temps_to_plot, aes(DATE,Temperature, colour=Method)) + geom_line(lwd=1.3) + ylab(&quot;Temperature (°C)&quot;) + xlab(&quot;Date&quot;) + ggtitle (&quot;Different temperature methods for Winters dataset&quot;) + scale_color_manual(labels = c(&quot;Observed&quot;, &quot;Ideal&quot;, &quot;Empirical&quot;), name=&quot;Temperature&quot;, values = c(&quot;slateblue&quot;, &quot;peru&quot;, &quot;mediumaquamarine&quot;)) + theme_bw(base_size = 14) Figure 8.4: Different temperature methods for Winters dataset "],["getting-temperature-data.html", "Chapter 9 Getting temperature data 9.1 Choose a location of interest and find the 25 closest weather stations using the handle_gsod function 9.2 Download weather data for the most promising station on the list 9.3 Convert the weather data into chillR format", " Chapter 9 Getting temperature data 9.1 Choose a location of interest and find the 25 closest weather stations using the handle_gsod function I chose a location in South Africa near Cape Town with a longitude of 18.8148 and a latitude of -33.9575, which can be seen in the leaflet below. coords &lt;- c(18.8148, -33.9575) Figure 9.1: Leaflet map for the selected location near Cape Town. The multi-functional handle_gsod function can be used to create a list of surrounding stations for a location of interest. The ten closest stations are shown in the list below and are also marked in the lower leaflet map. The apple marks the selected farm and the weather station icons symbolize the position of the stations. station_list&lt;-handle_gsod(action=&quot;list_stations&quot;, location=c(coords[1], coords[2]), time_interval=c(1977,2020), stations_to_choose_from = 25) Table 9.1: Table (shortened) for the station_list STATION.NAME CTRY Lat Long BEGIN END distance Overlap_years Perc_interval_covered CAPE TOWN INTL SF -33.965 18.602 19490101 20221215 19.61 44.00 100.00 CT-AWS SF -33.967 18.600 20041102 20221215 19.81 16.16 36.73 STRAND SF -34.150 18.850 20040525 20220328 21.66 16.60 37.74 ELGIN EXP FARM SF -34.139 19.023 19980301 20221215 27.93 22.84 51.90 YSTERPLANT(SAAFB) SF -33.900 18.500 20040224 20201230 29.71 16.85 38.30 PAARL SF -33.717 18.967 20040608 20221215 30.15 16.57 37.65 CAPE TOWN - PORTNET SF -33.900 18.433 19980301 20221215 35.79 22.84 51.90 MOLTENO RESERVIOR SF -33.933 18.417 20131210 20221215 36.80 7.06 16.04 SIMONSTOWN SF -34.200 18.433 19490101 19500625 44.31 0.00 0.00 ROBBEN ISLAND SF -33.800 18.367 20041005 20221215 44.87 16.24 36.91 library(leaflet) library(leafem) station_list &lt;- read.csv(&quot;data/station_list.csv&quot;) center &lt;- makeIcon( iconUrl = &quot;https://cdn-icons-png.flaticon.com/512/415/415682.png&quot;, iconWidth = 40, iconHeight = 40, iconAnchorX = 20, iconAnchorY = 20) stations &lt;- makeIcon( iconUrl = &quot;https://cdn-icons-png.flaticon.com/512/1753/1753451.png&quot;, iconWidth = 30, iconHeight = 30, iconAnchorX = 15, iconAnchorY = 15) html_legend &lt;- &quot;&lt;img src=&#39;https://cdn-icons-png.flaticon.com/512/415/415682.png&#39; style=&#39;width:30px;height:30px;&#39;&gt;Center&lt;br/&gt; &lt;img src=&#39;https://cdn-icons-png.flaticon.com/512/1753/1753451.png&#39; style=&#39;width:30px;height:30px;&#39;&gt;Stations&quot; m &lt;- leaflet(width = &quot;100%&quot;) m &lt;- addTiles(m) %&gt;% setView(m, lng = coords[1], lat = coords[2], zoom = 10) %&gt;% addLogo(&quot;https://symbols.getvecta.com/stencil_317/33_north-arrow-1.060831cef6.svg&quot;, position = &quot;bottomright&quot;, offset.x = 15, offset.y = 40, width = 100, height = 100) %&gt;% addProviderTiles(providers$Esri.WorldTopoMap, group = &quot;Topographical map&quot;) %&gt;% addProviderTiles(providers$Esri.WorldImagery, group = &quot;World imagery&quot;) %&gt;% addMarkers(m, lng=coords[1], lat= coords[2], popup=&quot;Center&quot;, icon = center) %&gt;% addLayersControl(baseGroups = c(&quot;Topographical map&quot;, &quot;World imagery&quot;), options = layersControlOptions(collapsed = FALSE)) %&gt;% addControl(html = html_legend, position = &quot;topleft&quot;) for(i in 1:10) { m &lt;- addMarkers(m, lng=station_list[i,5], lat=station_list[i,4], popup=station_list[i,2], icon = stations) } Show/Hide code Figure 9.2: Leaflet map with the weather stations of the station_list and the location of the farm in the center. 9.2 Download weather data for the most promising station on the list The first station seems to be most promising. It is the weather station of the airport of Cape Town, which has a high percentage of the interval covered. Data of other stations that are located directly on the coast are possibly even more affected by the coastal climate. Unfortunately, I could not find any information about the position of the weather station on the airport, but I hope it is representative enough for my case. In the next code chunk the handle_gsod function is used to download data with action input \"download_weather\". downloaded_weather&lt;-handle_gsod(action=&quot;download_weather&quot;, location=station_list$chillR_code[1], time_interval=c(1977,2020)) 9.3 Convert the weather data into chillR format When the data input has the right format, the handle_gsod function cleans the dataset, interpolates gaps and reports everything in a quality check addition. cleaned_weather&lt;-handle_gsod(downloaded_weather) Table 9.2: Table with data from cleaned_weather DATE Year Month Day Tmin Tmax Tmean Prec 1990-01-01 12:00:00 1990 1 1 16.500000 24.77778 19.94444 0.000 1990-01-02 12:00:00 1990 1 2 17.000000 24.00000 18.83333 0.254 1990-01-03 12:00:00 1990 1 3 13.000000 24.00000 19.72222 0.000 1990-01-04 12:00:00 1990 1 4 17.500000 26.61111 21.44444 0.000 1990-01-05 12:00:00 1990 1 5 18.611111 29.00000 22.33333 0.000 1990-01-06 12:00:00 1990 1 6 14.000000 29.00000 16.72222 3.556 1990-01-07 12:00:00 1990 1 7 12.888889 23.00000 18.00000 1.016 1990-01-08 12:00:00 1990 1 8 7.777778 32.00000 21.44444 0.000 1990-01-09 12:00:00 1990 1 9 14.611111 31.88889 19.33333 0.000 1990-01-10 12:00:00 1990 1 10 17.000000 24.77778 20.55556 0.000 "],["filling-gaps-in-temperature-records.html", "Chapter 10 Filling gaps in temperature records 10.1 Use chillR functions to find out how many gaps you have in this dataset (even if you have none, please still follow all further steps) 10.2 Create a list of the 25 closest weather stations using the handle_gsod function 10.3 Identify suitable weather stations for patching gaps 10.4 Download weather data for promising stations, convert them to chillR format and compile them in a list 10.5 Use the patch_daily_temperatures function to fill gaps 10.6 Investigate the results - have all gaps been filled? 10.7 If necessary, repeat until you have a dataset you can work with in further analyses", " Chapter 10 Filling gaps in temperature records 10.1 Use chillR functions to find out how many gaps you have in this dataset (even if you have none, please still follow all further steps) Before using the I first fix_weather()function, I created a gap in the dataset (2015/2016) to show the patching method on a real example. cleaned_weather &lt;- read.csv(&quot;data/CapeTown_chillR_weather.csv&quot;) CapeTown_weather_gap &lt;- cleaned_weather[-c(13890:14449),] require(chillR) fixed_weather &lt;- fix_weather(CapeTown_weather_gap) Table 10.1: Checking for gaps in the temperature record Season End_year Season_days Data_days Missing_Tmin Missing_Tmax Incomplete_days Perc_complete 1976/1977 1977 365 365 5 5 5 98.6 1977/1978 1978 365 365 0 0 0 100.0 1978/1979 1979 365 365 0 0 0 100.0 1979/1980 1980 366 366 1 1 1 99.7 1980/1981 1981 365 365 0 0 0 100.0 1981/1982 1982 365 365 0 0 0 100.0 1982/1983 1983 365 365 2 2 2 99.5 1983/1984 1984 366 366 0 0 0 100.0 1984/1985 1985 365 365 2 2 2 99.5 1985/1986 1986 365 365 0 0 0 100.0 1986/1987 1987 365 365 4 4 4 98.9 1987/1988 1988 366 366 0 0 0 100.0 1988/1989 1989 365 365 0 0 0 100.0 1989/1990 1990 365 365 0 0 0 100.0 1990/1991 1991 365 365 0 0 0 100.0 1991/1992 1992 366 366 0 0 0 100.0 1992/1993 1993 365 365 0 0 0 100.0 1993/1994 1994 365 365 0 0 0 100.0 1994/1995 1995 365 365 0 0 0 100.0 1995/1996 1996 366 366 0 0 0 100.0 1996/1997 1997 365 365 0 0 0 100.0 1997/1998 1998 365 365 0 0 0 100.0 1998/1999 1999 365 365 1 1 1 99.7 1999/2000 2000 366 366 1 0 1 99.7 2000/2001 2001 365 365 0 0 0 100.0 2001/2002 2002 365 365 0 0 0 100.0 2002/2003 2003 365 365 1 1 1 99.7 2003/2004 2004 366 366 0 0 0 100.0 2004/2005 2005 365 365 0 0 0 100.0 2005/2006 2006 365 365 0 0 0 100.0 2006/2007 2007 365 365 0 0 0 100.0 2007/2008 2008 366 366 0 0 0 100.0 2008/2009 2009 365 365 0 0 0 100.0 2009/2010 2010 365 365 0 0 0 100.0 2010/2011 2011 365 365 0 0 0 100.0 2011/2012 2012 366 366 0 0 0 100.0 2012/2013 2013 365 365 0 0 0 100.0 2013/2014 2014 365 365 0 0 0 100.0 2014/2015 2015 365 365 355 355 355 2.7 2015/2016 2016 366 366 205 205 205 44.0 2016/2017 2017 365 365 0 0 0 100.0 2017/2018 2018 365 365 0 0 0 100.0 2018/2019 2019 365 365 0 0 0 100.0 2019/2020 2020 366 366 1 1 1 99.7 10.2 Create a list of the 25 closest weather stations using the handle_gsod function I created a new patching_station_list for the time interval between 2014 and 2016, because we can see in the table above, that the artificial gap is in the years from 2014 till 2016. The new station list allow us to see, which weather stations have data for this gap. coords &lt;- c(18.8148, -33.9575) patching_station_list&lt;-handle_gsod(action=&quot;list_stations&quot;, location=c(coords[1], coords[2]), time_interval=c(2014,2016), stations_to_choose_from = 25) Table 10.2: List with suitable weather stations for patching chillR_code STATION.NAME CTRY Lat Long BEGIN END distance Overlap_years Perc_interval_covered 688160_99999 CAPE TOWN INTL SF -33.965 18.602 19490101 20230307 19.69 3 100 689990_99999 CT-AWS SF -33.967 18.600 20041102 20230307 19.88 3 100 689110_99999 STRAND SF -34.150 18.850 20040525 20220328 21.60 3 100 689250_99999 ELGIN EXP FARM SF -34.139 19.023 19980301 20230307 27.84 3 100 688270_99999 YSTERPLANT(SAAFB) SF -33.900 18.500 20040224 20201230 29.80 3 100 687130_99999 PAARL SF -33.717 18.967 20040608 20230307 30.17 3 100 688170_99999 CAPE TOWN - PORTNET SF -33.900 18.433 19980301 20230307 35.87 3 100 688190_99999 MOLTENO RESERVIOR SF -33.933 18.417 20131210 20230307 36.87 3 100 689140_99999 SIMONSTOWN SF -34.200 18.433 19490101 19500625 44.33 0 0 688130_99999 ROBBEN ISLAND SF -33.800 18.367 20041005 20230307 44.96 3 100 10.3 Identify suitable weather stations for patching gaps The CT-AWS’ station on the second position or the ELGIN EXP FARM station on position 4 could be suitable stations for patching gaps in our main data set. The CT-AWS station is characterized by a close distance to the location of interest and the ELGIN EXP FARM as a station that is less affected by coastal climate than other closer stations. PAARL serves as a last backup. 10.4 Download weather data for promising stations, convert them to chillR format and compile them in a list We download the weather data with this loop structure and the handle_gsod function. The data is downloaded and transformed. The previously selected stations are stored in positions_in_station_list. positions_in_station_list&lt;-c(2,4,6) patch_weather&lt;-list() for(i in 1:length(positions_in_station_list)) {patch_weather[[i]]&lt;-handle_gsod(handle_gsod(action=&quot;download_weather&quot;, location=patching_station_list$chillR_code[ positions_in_station_list[i]], time_interval=c(1977,2020)))[[1]]$weather names(patch_weather)[i]&lt;-patching_station_list$STATION.NAME[ positions_in_station_list[i]] } 10.5 Use the patch_daily_temperatures function to fill gaps patched &lt;- patch_daily_temperatures(weather = CapeTown_weather_gap, patch_weather = patch_weather) # ,max_mean_bias = 1, # max_stdev_bias = 2) The tables show the progress made during patching Table 10.3: Patch statistics for CT-AWS mean_bias stdev_bias filled gaps_remain Tmin -0.325 1.070 182 396 Tmax -1.376 2.287 182 395 Table 10.4: Patch statistics for ELGIN EXP FARM mean_bias stdev_bias filled gaps_remain Tmin 1.420 2.131 230 166 Tmax -0.681 2.953 229 166 Table 10.5: Patch statistics for PAARL mean_bias stdev_bias filled gaps_remain Tmin -0.604 2.283 147 19 Tmax -4.085 3.310 147 19 In a real patching scenarios, the standard derivation bias would be too high. It would be best to search for more suitable data for patching or use the data with an increased caution. 10.6 Investigate the results - have all gaps been filled? patched_fixed_weather &lt;- fix_weather(patched) final_patched_fixed_weather &lt;- fix_weather(patched_fixed_weather) Table 10.6: Quality check after the patching procedure Season End_year Season_days Data_days Missing_Tmin Missing_Tmax Incomplete_days Perc_complete 1976/1977 1977 365 365 5 5 5 98.6 1977/1978 1978 365 365 0 0 0 100.0 1978/1979 1979 365 365 0 0 0 100.0 1979/1980 1980 366 366 1 1 1 99.7 1980/1981 1981 365 365 0 0 0 100.0 1981/1982 1982 365 365 0 0 0 100.0 1982/1983 1983 365 365 2 2 2 99.5 1983/1984 1984 366 366 0 0 0 100.0 1984/1985 1985 365 365 2 2 2 99.5 1985/1986 1986 365 365 0 0 0 100.0 1986/1987 1987 365 365 4 4 4 98.9 1987/1988 1988 366 366 0 0 0 100.0 1988/1989 1989 365 365 0 0 0 100.0 1989/1990 1990 365 365 0 0 0 100.0 1990/1991 1991 365 365 0 0 0 100.0 1991/1992 1992 366 366 0 0 0 100.0 1992/1993 1993 365 365 0 0 0 100.0 1993/1994 1994 365 365 0 0 0 100.0 1994/1995 1995 365 365 0 0 0 100.0 1995/1996 1996 366 366 0 0 0 100.0 1996/1997 1997 365 365 0 0 0 100.0 1997/1998 1998 365 365 0 0 0 100.0 1998/1999 1999 365 365 1 1 1 99.7 1999/2000 2000 366 366 0 0 0 100.0 2000/2001 2001 365 365 0 0 0 100.0 2001/2002 2002 365 365 0 0 0 100.0 2002/2003 2003 365 365 1 1 1 99.7 2003/2004 2004 366 366 0 0 0 100.0 2004/2005 2005 365 365 0 0 0 100.0 2005/2006 2006 365 365 0 0 0 100.0 2006/2007 2007 365 365 0 0 0 100.0 2007/2008 2008 366 366 0 0 0 100.0 2008/2009 2009 365 365 0 0 0 100.0 2009/2010 2010 365 365 0 0 0 100.0 2010/2011 2011 365 365 0 0 0 100.0 2011/2012 2012 366 366 0 0 0 100.0 2012/2013 2013 365 365 0 0 0 100.0 2013/2014 2014 365 365 0 0 0 100.0 2014/2015 2015 365 365 2 2 2 99.5 2015/2016 2016 366 366 0 0 0 100.0 2016/2017 2017 365 365 0 0 0 100.0 2017/2018 2018 365 365 0 0 0 100.0 2018/2019 2019 365 365 0 0 0 100.0 2019/2020 2020 366 366 1 1 1 99.7 Table 10.7: Quality check after fixing left over gaps Season End_year Season_days Data_days Missing_Tmin Missing_Tmax Incomplete_days Perc_complete Length:44 Min. :1977 Min. :365.0 Min. :365.0 Min. :0 Min. :0 Min. :0 Min. :100 Class :character 1st Qu.:1988 1st Qu.:365.0 1st Qu.:365.0 1st Qu.:0 1st Qu.:0 1st Qu.:0 1st Qu.:100 Mode :character Median :1998 Median :365.0 Median :365.0 Median :0 Median :0 Median :0 Median :100 NA Mean :1998 Mean :365.2 Mean :365.2 Mean :0 Mean :0 Mean :0 Mean :100 NA 3rd Qu.:2009 3rd Qu.:365.2 3rd Qu.:365.2 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0 3rd Qu.:100 NA Max. :2020 Max. :366.0 Max. :366.0 Max. :0 Max. :0 Max. :0 Max. :100 10.7 If necessary, repeat until you have a dataset you can work with in further analyses The last single day gaps were fixed with the fixed_weather command. For further analyses, I will use the original dataset without the artificial gap. Gaps of single days are interpolated with fixed_weather. cleaned_weather &lt;- read.csv(&quot;data/CapeTown_chillR_weather.csv&quot;) "],["generating-temperature-scenarios.html", "Chapter 11 Generating temperature scenarios 11.1 For the location you chose for your earlier analyses, use chillR’s weather generator to produce 100 years of synthetic temperature data. 11.2 Calculate winter chill (in Chill Portions) for your synthetic weather, and illustrate your results as histograms and cumulative distributions. 11.3 Produce similar plots for the number of freezing hours (&lt;0°C) in April (or October, if your site is in the Southern Hemisphere) for your location of interest.", " Chapter 11 Generating temperature scenarios 11.1 For the location you chose for your earlier analyses, use chillR’s weather generator to produce 100 years of synthetic temperature data. We are using the temperature_generation function of the chillR package to generate 100 years of temperature that is calibrated on our input data weather_CapeTown. generated_temp&lt;-temperature_generation(weather_CapeTown, years=c(1990,2005), sim_years = c(2001,2100)) In the following chunk of code, we transformed and combined our datasets into one dataset that has the most suitable shape for the use with the ggplot function. temperatures&lt;-cbind(weather_CapeTown[ which(weather_CapeTown$Year %in% 1990:2005),], Data_source=&quot;observed&quot;) temperatures&lt;-rbind(temperatures[,c(&quot;Year&quot;,&quot;Month&quot;,&quot;Day&quot;,&quot;Tmin&quot;,&quot;Tmax&quot;,&quot;Data_source&quot;)], cbind(generated_temp[,c(&quot;Year&quot;,&quot;Month&quot;,&quot;Day&quot;,&quot;Tmin&quot;,&quot;Tmax&quot;)], Data_source=&quot;simulated&quot;)) temperatures[,&quot;Date&quot;]&lt;-as.Date(ISOdate(2000, temperatures$Month, temperatures$Day)) We plot the two different Data_sources in two plots for a better comparison. geom_smooth is a better method to display the data than geom_line because of our raw data. The plots are split with facet_wrap by their Data_source. ggplot(data=temperatures, aes(Date,Tmin)) + geom_smooth(aes(colour = factor(Year))) + facet_wrap(vars(Data_source)) + theme_bw(base_size = 20) + theme(legend.position = &quot;none&quot;) + scale_x_date(date_labels = &quot;%b&quot;) Figure 11.1: Geom_smooth() temperature plot to compare observed and simulated temperature data. 11.2 Calculate winter chill (in Chill Portions) for your synthetic weather, and illustrate your results as histograms and cumulative distributions. We apply the chilling() function to all of our temperatures that have the Data_source \"observed\" and \"simulated\". chill_observed&lt;-chilling( stack_hourly_temps( temperatures[which(temperatures$Data_source==&quot;observed&quot;),], latitude = coords[2]), Start_JDay = 121, # Beginning May, start of &quot;Winter&quot; in southern hemisphere End_JDay = 243) # End of August, end of &quot;Winter&quot; in southern hemisphere chill_simulated&lt;-chilling( stack_hourly_temps( temperatures[which(temperatures$Data_source==&quot;simulated&quot;),], latitude = coords[2]), Start_JDay = 121, End_JDay = 243) chill_comparison &lt;- cbind(chill_observed ,Data_source=&quot;observed&quot;) chill_simulated &lt;- cbind(chill_simulated ,Data_source=&quot;simulated&quot;) chill_comparison &lt;- rbind(chill_comparison, chill_simulated) #stacking simulated #and observed chill_comparison_full_seasons&lt;-chill_comparison[ which(chill_comparison$Perc_complete==100),] #only including complete years For plotting the chill we use geom_histogram, the color is defined by the Data_source. ggplot(chill_comparison_full_seasons, aes(x=Chill_portions)) + geom_histogram(binwidth=1,aes(fill = factor(Data_source))) + theme_bw(base_size = 20) + labs(fill = &quot;Data source&quot;) + xlab(&quot;Chill accumulation (Chill Portions)&quot;) + ylab(&quot;Frequency&quot;) Figure 11.2: A histogram that shows the chill portions for the observed and simulated data. We can calculate and plot the Safe Winter Chill (SWC) as seen below. In 9 out of 10 years the amount of chilling can be expected to be above this value. chill_simulations&lt;-chill_comparison_full_seasons[ which(chill_comparison_full_seasons$Data_source==&quot;simulated&quot;),] ggplot(chill_simulations, aes(x=Chill_portions)) + stat_ecdf(geom = &quot;step&quot;,lwd=1.5,col=&quot;blue&quot;) + ylab(&quot;Cumulative probability&quot;) + xlab(&quot;Chill accumulation (in Chill Portions)&quot;) + geom_hline(yintercept = 0.1, colour=&quot;red&quot;) + theme_bw(base_size = 20) Figure 11.3: The accumulated chill in Chill Portions. The red horizontal line marks the Safe Winter Chill. quantile(chill_simulations$Chill_portions, 0.1) ## 10% ## 32.30273 11.3 Produce similar plots for the number of freezing hours (&lt;0°C) in April (or October, if your site is in the Southern Hemisphere) for your location of interest. First, I created a freezing_hours step_model(), that can be used as a model in the tempResponse() function. After that I applied it on the data in the same way as seen before, I also prepare the data in the same way and also filter all incomplete seasons. freezing_hours_steps &lt;- data.frame(lower=c(-1000,0), upper=c(0,1000), weight=c(1,0)) freezing_hours &lt;- function(x) step_model(x,freezing_hours_steps) CapeTown_temperature &lt;- cbind(weather_CapeTown[ which(weather_CapeTown$Year %in% 1990:2005),]) observed_temperature &lt;- stack_hourly_temps(CapeTown_temperature[,c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;, &quot;Tmin&quot;, &quot;Tmax&quot;)], latitude = coords[2])$hourtemps simulated_temperature &lt;- stack_hourly_temps(generated_temp[,c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;, &quot;Tmin&quot;, &quot;Tmax&quot;)], latitude = coords[2])$hourtemps freezing_observed &lt;- tempResponse(make_JDay(observed_temperature), Start_JDay = 274, End_JDay = 304, models=list(Freezing_Hours=freezing_hours)) freezing_simulated &lt;- tempResponse(make_JDay(simulated_temperature), Start_JDay = 274, End_JDay = 304, models=list(Freezing_Hours=freezing_hours)) freezing_comparison &lt;- cbind(freezing_observed, Data_source=&quot;observed&quot;) freezing_simulated &lt;- cbind(freezing_simulated, Data_source=&quot;simulated&quot;) freezing_comparison &lt;- rbind(freezing_comparison, freezing_simulated) freezing_comparison_full_seasons&lt;-freezing_comparison[ which(freezing_comparison$Perc_complete==100),] We can plot the data in the same way as seen before in Figure 11.2. This time we can use the freezing_comparison dataset as input for the ggplot function. ggplot(freezing_comparison, aes(x=Freezing_Hours)) + geom_histogram(binwidth=1,aes(fill = factor(Data_source))) + theme_bw(base_size = 20) + labs(fill = &quot;Data source&quot;) + xlab(&quot;Freezing Hours&quot;) + ylab(&quot;Frequency&quot;) Figure 11.4: A histogram that shows the freezing hours for the observed and simulated data. My chosen location is near Cape Town, South Africa. As we can see in Figure 11.4, freezing hours are nearly non existent. Calculating something like a “Safe Freezing Hours” is not possible. freezing_simulations&lt;-freezing_comparison_full_seasons[ which(freezing_comparison_full_seasons$Data_source==&quot;simulated&quot;),] ggplot(freezing_simulations, aes(x=Freezing_Hours)) + stat_ecdf(geom = &quot;step&quot;,lwd=1.5,col=&quot;blue&quot;) + ylab(&quot;Cumulative probability&quot;) + xlab(&quot;Freeze hours accumulation&quot;) + theme_bw(base_size = 20) Figure 11.5: The graph for accumulated freeze hours. "],["saving-and-loading-data.html", "Chapter 12 Saving and Loading Data", " Chapter 12 Saving and Loading Data The chapter conveyed some important tips to reduce the knitting and processing time for the Markdown files with chunk attributes like echo=FALSE and eval=FALSE. We are able to hide some parts of our code or show code to the reader, without executing it in the background. Figure 12.1: Screenshot of a code chunk with options explained above "],["historic-temperature-scenarios.html", "Chapter 13 Historic temperature scenarios 13.1 For the location you chose for previous exercises, produce historic temperature scenarios representing several years of the historic record (your choice). 13.2 Produce chill distributions for these scenarios and plot them.", " Chapter 13 Historic temperature scenarios 13.1 For the location you chose for previous exercises, produce historic temperature scenarios representing several years of the historic record (your choice). After creating a baseline scenario for 2001, we can use the temperature_scenario_from_records function to create absolute scenarios for the years c(1985, 1995, 2005, 2015). The absolute scenarios are adjusted to the baseline to be used in the temperature_generation() function. require(chillR) scenario_2001&lt;-temperature_scenario_from_records(weather=weather_CapeTown, year=2001) all_past_scenarios &lt;- temperature_scenario_from_records( weather=weather_CapeTown, year=c(1985, 1995, 2005, 2015)) #creating absolute scenarios for those years adjusted_scenarios &lt;- temperature_scenario_baseline_adjustment( baseline=scenario_2001, temperature_scenario = all_past_scenarios) #transform to relative with bl 96 all_past_scenario_temps &lt;- temperature_generation( weather=weather_CapeTown, years=c(1982,2020), sim_years=c(2001,2100), temperature_scenario = adjusted_scenarios) Baseline 2001 Absolute 1985 Adjusted relative 1985 Table 13.1: Three tables to display the adjustment of the scenario for 1985 with the baseline of 2001 Tmin Tmax 16.152210 27.87210 16.226086 28.42761 14.440561 26.94397 11.958025 24.71802 9.854958 21.64432 7.680864 19.13988 6.839785 18.38471 7.663321 18.86260 9.015185 20.48383 10.818518 23.17312 13.148272 24.55012 15.325568 26.77467 Tmin Tmax 15.993429 26.63172 15.776060 26.95791 14.495938 25.83405 12.238395 23.62877 9.708542 21.17754 7.467284 18.93593 7.288829 18.21661 7.906810 18.82796 9.057963 19.90099 10.928076 21.95842 13.282407 24.03389 15.062067 25.37497 Tmin Tmax -0.1587814 -1.2403823 -0.4500251 -1.4696999 0.0553763 -1.1099164 0.2803704 -1.0892593 -0.1464158 -0.4667861 -0.2135802 -0.2039506 0.4490442 -0.1681004 0.2434886 -0.0346476 0.0427778 -0.5828395 0.1095579 -1.2146953 0.1341358 -0.5162346 -0.2635006 -1.3997013 13.2 Produce chill distributions for these scenarios and plot them. We apply the tempResponse_daily_list function to our generated data and add a column with the scenario year and stack all 4 scenarios vertically. In actual_chill, we calculate the chilling for our original data. chill_hist_scenario_list &lt;- tempResponse_daily_list(all_past_scenario_temps, latitude=coords[2], Start_JDay = 121, End_JDay = 243) scenarios&lt;-names(chill_hist_scenario_list)[1:4] all_scenarios&lt;-chill_hist_scenario_list[[scenarios[1]]] all_scenarios[,&quot;scenario&quot;] &lt;- as.numeric(scenarios[1]) for (sc in scenarios[2:4]) all_scenarios&lt;-rbind(all_scenarios, cbind(chill_hist_scenario_list[[sc]],scenario=as.numeric(sc))) all_scenarios &lt;- all_scenarios[which(all_scenarios$Perc_complete==100),] actual_chill &lt;- tempResponse_daily_list(weather_CapeTown,latitude=coords[2], Start_JDay = 121, End_JDay = 243)[[1]] actual_chill &lt;- actual_chill[which(actual_chill$Perc_complete==100),] This time, we use the geom_violin to display the distribution of chill for our scenario years. With geom_point, we can add the data from the actual_chill to our plot. require(ggplot2) ggplot(data=all_scenarios,aes(scenario,Chill_Portions, fill=factor(scenario))) + geom_violin() + ylab(&quot;Chill accumulation (Chill Portions)&quot;) + theme_bw(base_size=15) + scale_x_discrete(name =&quot;Scenario year&quot;, limits=unique(all_scenarios$scenario)) + ylim(c(0,90)) + geom_point(data=actual_chill, aes(End_year,Chill_Portions,fill=&quot;blue&quot;), col=&quot;blue&quot;,show.legend = FALSE) + scale_fill_discrete(name=&quot;Scenario&quot;, breaks = unique(all_scenarios$scenario)) Figure 13.1: Violin charts that display the chill accumulation for the simulated years of 1985,1995,2005,2015. The blue points represent the actual chill measured during this time "],["future-temperature-scenarios.html", "Chapter 14 Future temperature scenarios 14.1 Analyze the historic and future impact of climate change on three agroclimatic metrics of your choice, for the location you’ve chosen for your earlier analyses.", " Chapter 14 Future temperature scenarios 14.1 Analyze the historic and future impact of climate change on three agroclimatic metrics of your choice, for the location you’ve chosen for your earlier analyses. We are downloading the temperature scenarios for the baseline between 1977 and 2005 for two RCPs with the help of a loop. The baseline is selected in a way to achieve an integer as median, which is in this case 1991. The data is directly saved afterwards in the ClimateWizard directory in the data directory. RCPs&lt;-c(&quot;rcp45&quot;,&quot;rcp85&quot;) Times&lt;-c(2050,2085) for(RCP in RCPs) for(Time in Times) {start_year &lt;- Time-15 end_year &lt;- Time+15 clim_scen &lt;-getClimateWizardData( c(longitude = coords[1],latitude = coords[2]), RCP, start_year, end_year, temperature_generation_scenarios = TRUE, baseline =c(1977, 2005), metric = &quot;monthly_min_max_temps&quot;, GCMs = &quot;all&quot;) save_temperature_scenarios(clim_scen, &quot;data/ClimateWizard&quot;, paste0(&quot;CapeTown_futures_&quot;,Time,&quot;_&quot;,RCP))} In the next step we need to adjust the baseline of our observed data to the baseline we want to use for the future data. The temperature_scenario_baseline_adjustment command is made for this purpose. scenario_1991&lt;-temperature_scenario_from_records(weather_CapeTown,1991) #median of 1977 and 2005 scenario_1998&lt;-temperature_scenario_from_records(weather_CapeTown,1998) #median of 1977 and 2019 adjustment_scenario&lt;-temperature_scenario_baseline_adjustment(scenario_1998,scenario_1991) In another for loop, we can use the load_ClimateWizard_scenarios function to load the previously downloaded climate scenarios. The climate scenarios are adjusted to the baseline and the weather is then generated with the temperature_generation function. After generation the data is saved to be easier accessed for the next use case. RCPs&lt;-c(&quot;rcp45&quot;,&quot;rcp85&quot;) Times&lt;-c(2050,2085) for(RCP in RCPs) for(Time in Times) { clim_scen&lt;-load_ClimateWizard_scenarios( &quot;data/climateWizard&quot;, paste0(&quot;CapeTown_futures_&quot;,Time,&quot;_&quot;,RCP)) clim_scen_adjusted&lt;- temperature_scenario_baseline_adjustment( baseline_temperature_scenario=adjustment_scenario, temperature_scenario=clim_scen) Temps&lt;-temperature_generation( weather=weather_CapeTown, years=c(1977,2019), sim_years=c(2001,2101), temperature_scenario = clim_scen_adjusted) save_temperature_scenarios( Temps, &quot;data/Weather&quot;, paste0(&quot;CapeTown_&quot;,Time,&quot;_&quot;,RCP)) } We create additional historic scenarios, that we can later use for display in the plot. Before generating the temperature, the scenarios are adjusted to the baseline. After generation, the newly created weather is saved in the same directory as the previously generated future weather data. all_past_scenarios&lt;-temperature_scenario_from_records( weather=weather_CapeTown, year=c(1980,1990,2000,2010)) adjusted_scenarios&lt;-temperature_scenario_baseline_adjustment( baseline=scenario_1998, temperature_scenario = all_past_scenarios) #ensure relative climate all_past_scenario_temps&lt;-temperature_generation( weather=weather_CapeTown, years=c(1977,2019), sim_years=c(2001,2101), temperature_scenario = adjusted_scenarios) save_temperature_scenarios( all_past_scenario_temps, &quot;data/Weather&quot;, &quot;CapeTown_historic&quot;) With tempResponse_daily_list we are able to apply our three agroclimatic metrics to our historic data, which is loaded at once. For the frost model, we create one by ourselves with the help of the step_model function. frost_model&lt;-function(x) step_model(x,data.frame( lower=c(-1000,0), upper=c(0,1000), weight=c(1,0))) models&lt;-list(Chill_CP=Dynamic_Model,Heat_GDH=GDH,Frost_H=frost_model) Temps&lt;-load_temperature_scenarios(&quot;data/Weather&quot;,&quot;CapeTown_historic&quot;) chill_past_scenarios&lt;-tempResponse_daily_list( Temps, latitude=coords[2], Start_JDay = 121, End_JDay = 243, models=models, misstolerance = 10) chill_observed&lt;-tempResponse_daily_list( weather_CapeTown, latitude=coords[2], Start_JDay = 121, End_JDay = 243, models=models, misstolerance = 10) save_temperature_scenarios(chill_past_scenarios, &quot;data/chill&quot;, &quot;CapeTown_historic&quot;) save_temperature_scenarios(chill_observed, &quot;data/chill&quot;, &quot;CapeTown_observed&quot;) Prior to plotting, we need to bring the chill data in the right shape with the help of the make_climate_scenario function. chills &lt;- make_climate_scenario( chill_past_scenarios, caption = &quot;Historic&quot;, historic_data = chill_observed, time_series = TRUE) In the next code chunk, we use the same for loop, to cycle trough our data structure to apply the agroclimatic metrics to the future data. Also this data is saved afterwards. for(RCP in RCPs) for(Time in Times) { Temps&lt;-load_temperature_scenarios( &quot;data/Weather&quot;, paste0(&quot;CapeTown_&quot;,Time,&quot;_&quot;,RCP)) chill&lt;-tempResponse_daily_list( Temps, latitude= coords[2], Start_JDay = 121, End_JDay = 243, models=models, misstolerance = 10) save_temperature_scenarios( chill, &quot;data/chill&quot;, paste0(&quot;CapeTown_&quot;,Time,&quot;_&quot;,RCP)) } We again use the make_climate_scenario function and the same loop structure to bring the last parts of our dataset in shape. With add_to we can combine the future chill data with the previously created historic chill data. for(RCP in RCPs) for(Time in Times) { chill&lt;-load_temperature_scenarios( &quot;data/chill&quot;, paste0(&quot;CapeTown_&quot;,Time,&quot;_&quot;,RCP)) if(RCP==&quot;rcp45&quot;) RCPcaption &lt;- &quot;RCP4.5&quot; if(RCP==&quot;rcp85&quot;) RCPcaption &lt;- &quot;RCP8.5&quot; if(Time==&quot;2050&quot;) Time_caption &lt;- &quot;2050&quot; if(Time==&quot;2085&quot;) Time_caption &lt;- &quot;2085&quot; chills &lt;-make_climate_scenario( chill, caption =c(RCPcaption, Time_caption), add_to = chills) } The three graphs show the development of the three selected agroclimatic metrics of the past and for our forecast for 2050 and 2085 for two different RCPs. These graphs are made with base R in the next chapter we take the same data and plot those graphs with the help of the ggplot2 package. I will then also analyse the graphs in detail. Figure 14.1: plot_climate_scenarios plot for Chill Portions for future temperature scenarios Figure 14.2: plot_climate_scenarios plot for Growing Degree Hours for future temperature scenarios Figure 14.3: plot_climate_scenarios plot for Frost hours for future temperature scenarios "],["plotting-future-scenarios.html", "Chapter 15 Plotting future scenarios 15.1 Produce similar plots for the weather station you selected for earlier exercises.", " Chapter 15 Plotting future scenarios 15.1 Produce similar plots for the weather station you selected for earlier exercises. library(chillR) library(reshape2) library(ggplot2) library(ggpmisc) library(patchwork) library(kableExtra) chill_past_scenarios &lt;- load_temperature_scenarios( &quot;data/chill&quot;, &quot;CapeTown_historic&quot;) chill_observed &lt;- load_temperature_scenarios( &quot;data/chill&quot;, &quot;CapeTown_observed&quot;) chills &lt;-make_climate_scenario( chill_past_scenarios, caption = &quot;Historic&quot;, historic_data = chill_observed, time_series = TRUE) RCPs &lt;- c(&quot;rcp45&quot;,&quot;rcp85&quot;) Times &lt;- c(2050,2085) for(RCP in RCPs) for(Time in Times) {chill &lt;- load_temperature_scenarios( &quot;data/chill&quot;, paste0(&quot;CapeTown_&quot;,Time,&quot;_&quot;,RCP)) if(RCP == &quot;rcp45&quot;) RCPcaption &lt;- &quot;RCP4.5&quot; if(RCP == &quot;rcp85&quot;) RCPcaption &lt;- &quot;RCP8.5&quot; if(Time == &quot;2050&quot;) Time_caption &lt;- &quot;2050&quot; if(Time == &quot;2085&quot;) Time_caption &lt;- &quot;2085&quot; chills &lt;- make_climate_scenario( chill, caption = c(RCPcaption, Time_caption), add_to = chills) } for(nam in names(chills[[1]]$data)) { ch&lt;-chills[[1]]$data[[nam]] ch[,&quot;GCM&quot;] &lt;- &quot;none&quot; ch[,&quot;RCP&quot;] &lt;- &quot;none&quot; ch[,&quot;Year&quot;] &lt;- as.numeric(nam) if(nam == names(chills[[1]]$data)[1]) past_simulated &lt;- ch else past_simulated &lt;- rbind(past_simulated,ch) } past_simulated[&quot;Scenario&quot;] &lt;- &quot;Historic&quot; past_observed &lt;- chills[[1]][[&quot;historic_data&quot;]] #simplyfying pointer for(i in 2:length(chills)) for(nam in names(chills[[i]]$data)) {ch&lt;-chills[[i]]$data[[nam]] ch[,&quot;GCM&quot;] &lt;- nam ch[,&quot;RCP&quot;] &lt;- chills[[i]]$caption[1] ch[,&quot;Year&quot;] &lt;- chills[[i]]$caption[2] if(i == 2&amp;nam == names(chills[[i]]$data)[1]) future_data &lt;- ch else future_data &lt;- rbind(future_data,ch) } This is are the three different dataframes we need to plot the full graph. For the historic data, we have the metrics calculated from the simulated weather and extra data for the geom_point() plot. The dataframe for the future part of the plot, contains all the information, so that we can easily split it into the subplots with the facet_grid() function. Table 15.1: Final dataframe for plotting the historic boxplots Season End_year Season_days Data_days Perc_complete Chill_CP Heat_GDH Frost_H GCM RCP Year Scenario 2000/2001 2001 123 123 100 40.01651 25461.99 0 none none 1980 Historic 2001/2002 2002 123 123 100 36.19151 26279.66 0 none none 1980 Historic 2002/2003 2003 123 123 100 43.94242 25793.20 0 none none 1980 Historic 2003/2004 2004 123 123 100 42.50185 24986.08 0 none none 1980 Historic 2004/2005 2005 123 123 100 44.96755 25097.39 0 none none 1980 Historic Table 15.2: Final dataframe for plotting the historic point plot Season End_year Season_days Data_days Interpolated_days Perc_complete Chill_CP Heat_GDH Frost_H 1976/1977 1977 123 123 1 99.18699 47.35204 24188.96 0 1977/1978 1978 123 123 0 100.00000 38.61035 25274.43 4 1978/1979 1979 123 123 0 100.00000 39.52377 26679.28 0 1979/1980 1980 123 123 0 100.00000 45.20518 23928.95 0 1980/1981 1981 123 123 0 100.00000 58.06163 20428.03 2 Table 15.3: Final dataframe for plotting the future plots Season End_year Season_days Data_days Perc_complete Chill_CP Heat_GDH Frost_H GCM RCP Year 2000/2001 2001 123 123 100 26.74566 27971.20 0 bcc-csm1-1 RCP4.5 2050 2001/2002 2002 123 123 100 26.81553 29202.97 1 bcc-csm1-1 RCP4.5 2050 2002/2003 2003 123 123 100 28.98867 28645.35 0 bcc-csm1-1 RCP4.5 2050 2003/2004 2004 123 123 100 32.31504 27616.76 0 bcc-csm1-1 RCP4.5 2050 2004/2005 2005 123 123 100 33.57207 28098.57 0 bcc-csm1-1 RCP4.5 2050 plot_scenarios_gg &lt;- function(past_observed, past_simulated, future_data, metric, axis_label) { rng &lt;- range(past_observed[[metric]], past_simulated[[metric]], future_data[[metric]]) past_plot &lt;- ggplot() + geom_boxplot(data = past_simulated, aes_string(&quot;as.numeric(Year)&quot;,metric,group = &quot;Year&quot;), fill = &quot;skyblue&quot;) + scale_y_continuous(limits = c(0, round(round(1.1*rng[2])))) + labs(x = &quot;Year&quot;, y = axis_label) + facet_grid(~ Scenario) + theme_bw(base_size = 15) + theme(strip.background = element_blank(), strip.text = element_text(face = &quot;bold&quot;), axis.text.x = element_text(angle=45, hjust=1)) + geom_point(data = past_observed, aes_string(&quot;End_year&quot;,metric), col=&quot;blue&quot;) future_plot_list &lt;- list() for(y in c(2050,2085)) { future_plot_list[[which(y == c(2050,2085))]] &lt;- ggplot(data= future_data[which(future_data$Year == y),]) + geom_boxplot(aes_string(&quot;GCM&quot;, metric, fill=&quot;GCM&quot;)) + facet_wrap(vars(RCP)) + scale_x_discrete(labels = NULL, expand = expansion(add = 1)) + scale_y_continuous(limits = c(0, round(round(1.1*rng[2])))) + geom_text_npc(aes(npcx = &quot;center&quot;, npcy = &quot;top&quot;, label = Year), size = 5) + theme_bw(base_size = 15) + theme(axis.ticks.y = element_blank(), axis.text = element_blank(), axis.title = element_blank(), legend.position = &quot;bottom&quot;, legend.margin = margin(0, 0, 0, 0, &quot;cm&quot;), legend.background = element_rect(), strip.background = element_blank(), strip.text = element_text(face = &quot;bold&quot;), legend.box.spacing = unit(0, &quot;cm&quot;), plot.subtitle = element_text(hjust = 0.5, vjust = -1, size = 15 * 1.05, face = &quot;bold&quot;)) } plot&lt;- (past_plot + future_plot_list + plot_layout(guides = &quot;collect&quot;, widths = c(1,rep(1.8,length(future_plot_list))))) + plot_annotation(theme = theme(plot.title = element_text(size = 24))) &amp; theme(legend.position = &quot;bottom&quot;, legend.text = element_text(size=8), legend.title = element_text(size=10), axis.title.x = element_blank()) } The plots show the calculated Growing Degree Hours, Chill Portions and Frost Hours for the historic records and four simulated future scenarios. The simulation reveals that Growing Degree Hours will increase, while the Chill Portions decrease. Some GCMs contradict each other, but due to the use of this GCM ensemble we can still recognize clear trends. In the past, there were only a few frost hours, which will most likely not change in the future except for a few outliers. CapeTown_Heat_GDH &lt;- plot_scenarios_gg(past_observed=past_observed, past_simulated=past_simulated, future_data=future_data, metric=&quot;Heat_GDH&quot;, axis_label=&quot;Heat (in Growing Degree Hours)&quot;) CapeTown_Heat_GDH CapeTown_Chill_CP &lt;- plot_scenarios_gg(past_observed=past_observed, past_simulated=past_simulated, future_data=future_data, metric=&quot;Chill_CP&quot;, axis_label=&quot;Chill (in Chill Portions)&quot;) CapeTown_Chill_CP CapeTown_Frost_H &lt;- plot_scenarios_gg(past_observed=past_observed, past_simulated=past_simulated, future_data=future_data, metric=&quot;Frost_H&quot;, axis_label=&quot;Frost duration (in hours)&quot;) CapeTown_Frost_H ggsave(&quot;plots/CapeTownPlot_Heat_GDH.png&quot;, CapeTown_Heat_GDH, width = 10, height = 5, dpi = 800) ggsave(&quot;plots/CapeTownPlot_Chill_CP.png&quot;, CapeTown_Chill_CP, width = 10, height = 5, dpi = 800) ggsave(&quot;plots/CapeTownPlots_Frost_H_RED.png&quot;, CapeTown_Frost_H, width = 10, height = 5, dpi = 800) "],["chill-model-comparison.html", "Chapter 16 Chill model comparison 16.1 Perform a similar analysis for the location you’ve chosen for your exercises. 16.2 Make a heat map illustrating past and future changes in Safe Winter Chill, relative to a past scenario, for the 13 chill models used here. 16.3 Produce an animated line plot of your results (summarizing Safe Winter Chill across all the GCMs).", " Chapter 16 Chill model comparison 16.1 Perform a similar analysis for the location you’ve chosen for your exercises. In the first step, we rename all the chill metrics of the dormancyR package. And assign also names for the plotting. hourly_models &lt;- list(Chilling_units = chilling_units, Low_chill = low_chill_model, Modified_Utah = modified_utah_model, North_Carolina = north_carolina_model, Positive_Utah = positive_utah_model, Chilling_Hours = Chilling_Hours, Utah_Chill_Units = Utah_Model, Chill_Portions = Dynamic_Model) daily_models&lt;-list(Rate_of_Chill = rate_of_chill, Chill_Days = chill_days, Exponential_Chill = exponential_chill, Triangula_Chill_Haninnen = triangular_chill_1, Triangular_Chill_Legave = triangular_chill_2) metrics&lt;-c(names(daily_models),names(hourly_models)) model_labels=c(&quot;Rate of Chill&quot;, &quot;Chill Days&quot;, &quot;Exponential Chill&quot;, &quot;Triangular Chill (Häninnen)&quot;, &quot;Triangular Chill (Legave)&quot;, &quot;Chilling Units&quot;, &quot;Low-Chill Chill Units&quot;, &quot;Modified Utah Chill Units&quot;, &quot;North Carolina Chill Units&quot;, &quot;Positive Utah Chill Units&quot;, &quot;Chilling Hours&quot;, &quot;Utah Chill Units&quot;, &quot;Chill Portions&quot;) Table 16.1: List with all Chill metrics Metric Function.name Rate of Chill Rate_of_Chill Chill Days Chill_Days Exponential Chill Exponential_Chill Triangular Chill (Häninnen) Triangula_Chill_Haninnen Triangular Chill (Legave) Triangular_Chill_Legave Chilling Units Chilling_units Low-Chill Chill Units Low_chill Modified Utah Chill Units Modified_Utah North Carolina Chill Units North_Carolina Positive Utah Chill Units Positive_Utah Chilling Hours Chilling_Hours Utah Chill Units Utah_Chill_Units Chill Portions Chill_Portions In the next code chunk, we apply the whole list of chill metrics to all our past scenarios. And save our newly calculated data as CapeTown_multichill_historic CapeTown_temps&lt;-read_tab(&quot;data/CapeTown_weather.csv&quot;) Temps&lt;-load_temperature_scenarios(&quot;data/Weather&quot;,&quot;CapeTown_historic&quot;) Start_JDay&lt;-121 End_JDay&lt;-243 daily_models_past_scenarios&lt;-tempResponse_list_daily( Temps, Start_JDay = Start_JDay, End_JDay = End_JDay, models=daily_models) daily_models_past_scenarios&lt;-lapply( daily_models_past_scenarios, function(x) x[which(x$Perc_complete&gt;90),]) hourly_models_past_scenarios&lt;-tempResponse_daily_list( Temps, latitude= coords[2], Start_JDay = Start_JDay, End_JDay = End_JDay, models=hourly_models, misstolerance = 10) past_scenarios&lt;-daily_models_past_scenarios past_scenarios&lt;-lapply( names(past_scenarios), function(x) cbind(past_scenarios[[x]], hourly_models_past_scenarios[[x]][,names(hourly_models)])) names(past_scenarios)&lt;-names(daily_models_past_scenarios) daily_models_observed&lt;-tempResponse_daily( CapeTown_temps, Start_JDay = Start_JDay, End_JDay = End_JDay, models=daily_models) daily_models_observed&lt;- daily_models_observed[which(daily_models_observed$Perc_complete&gt;90),] hourly_models_observed&lt;-tempResponse_daily_list( CapeTown_temps, latitude= coords[2], Start_JDay = Start_JDay, End_JDay = End_JDay, models=hourly_models, misstolerance = 10) past_observed&lt;-cbind( daily_models_observed, hourly_models_observed[[1]][,names(hourly_models)]) save_temperature_scenarios(past_scenarios, &quot;data/chill&quot;, &quot;CapeTown_multichill_historic&quot;) write.csv(past_observed, &quot;data/chill/CapeTown_multichill_observed.csv&quot;, row.names=FALSE) In the next code chunk, we apply the whole list of chill metrics to all our future scenarios. And save our newly calculated data in multiple files depending on their RCP and date. RCPs&lt;-c(&quot;rcp45&quot;,&quot;rcp85&quot;) Times&lt;-c(2050,2085) for(RCP in RCPs) for(Time in Times) { Temps&lt;-load_temperature_scenarios( &quot;data/Weather&quot;, paste0(&quot;CapeTown_&quot;,Time,&quot;_&quot;,RCP)) daily_models_future_scenarios&lt;-tempResponse_list_daily( Temps, Start_JDay = Start_JDay, End_JDay = End_JDay, models=daily_models) daily_models_future_scenarios&lt;-lapply( daily_models_future_scenarios, function(x) x[which(x$Perc_complete&gt;90),]) hourly_models_future_scenarios&lt;- tempResponse_daily_list( Temps, latitude= coords[2], Start_JDay = Start_JDay, End_JDay = End_JDay, models=hourly_models, misstolerance = 10) future_scenarios&lt;-daily_models_future_scenarios future_scenarios&lt;-lapply( names(future_scenarios), function(x) cbind(future_scenarios[[x]], hourly_models_future_scenarios[[x]][,names(hourly_models)])) names(future_scenarios)&lt;-names(daily_models_future_scenarios) chill&lt;-future_scenarios save_temperature_scenarios( chill, &quot;data/chill&quot;, paste0(&quot;CapeTown_multichill_&quot;,Time,&quot;_&quot;,RCP)) } chill_past_scenarios&lt;-load_temperature_scenarios( &quot;data/chill&quot;, &quot;CapeTown_multichill_historic&quot;) chill_observed&lt;-read_tab(&quot;data/chill/CapeTown_multichill_observed.csv&quot;) chills &lt;-make_climate_scenario(chill_past_scenarios, caption = &quot;Historic&quot;, historic_data = chill_observed, time_series = TRUE) RCPs&lt;-c(&quot;rcp45&quot;,&quot;rcp85&quot;) Times&lt;-c(2050,2085) for(RCP in RCPs) for(Time in Times) { chill&lt;-load_temperature_scenarios( &quot;data/chill&quot;, paste0(&quot;CapeTown_multichill_&quot;,Time,&quot;_&quot;,RCP)) if(RCP==&quot;rcp45&quot;) RCPcaption &lt;- &quot;RCP4.5&quot; if(RCP==&quot;rcp85&quot;) RCPcaption &lt;- &quot;RCP8.5&quot; if(Time==&quot;2050&quot;) Time_caption &lt;- &quot;2050&quot; if(Time==&quot;2085&quot;) Time_caption &lt;- &quot;2085&quot; chills &lt;-make_climate_scenario(chill, caption =c(RCPcaption,Time_caption), add_to = chills) } 16.2 Make a heat map illustrating past and future changes in Safe Winter Chill, relative to a past scenario, for the 13 chill models used here. for(i in 1:length(chills)) {ch&lt;-chills[[i]] if(ch$caption[1]==&quot;Historic&quot;) {GCMs&lt;-rep(&quot;none&quot;,length(names(ch$data))) RCPs&lt;-rep(&quot;none&quot;,length(names(ch$data))) Years&lt;-as.numeric(ch$labels) Scenario&lt;-rep(&quot;Historic&quot;,length(names(ch$data)))} else {GCMs&lt;-names(ch$data) RCPs&lt;-rep(ch$caption[1],length(names(ch$data))) Years&lt;-rep(as.numeric(ch$caption[2]),length(names(ch$data))) Scenario&lt;-rep(&quot;Future&quot;,length(names(ch$data)))} for(nam in names(ch$data)) {for(met in metrics) {temp_res&lt;-data.frame(Metric=met, GCM=GCMs[which(nam==names(ch$data))], RCP=RCPs[which(nam==names(ch$data))], Year=Years[which(nam==names(ch$data))], Result=quantile(ch$data[[nam]][,met],0.1), Scenario=Scenario[which(nam==names(ch$data))]) if(i==1&amp;nam==names(ch$data)[1]&amp;met==metrics[1]) results&lt;-temp_res else results&lt;-rbind(results,temp_res) results }}} for(met in metrics) results[which(results$Metric==met),&quot;SWC&quot;]&lt;- results[which(results$Metric==met),&quot;Result&quot;]/ results[which(results$Metric==met&amp;results$Year==1980),&quot;Result&quot;]-1 We first create a single heatmap for the SWC for all future scenarios together. After that we separate the plots by RCP and year with the facet_grid() function. To prepare the graph for the combination with the graph with data from the past, we need to adjust the size and apply some theme changes. Labels and an adjusted color map help for a better understanding. require(ggplot2) require(patchwork) require(colorRamps) rng = range(results$SWC) p_future &lt;- ggplot(results[which(!results$GCM==&quot;none&quot;),], aes(GCM, y=factor(Metric, levels=metrics), fill = SWC)) + geom_tile() + facet_grid(RCP ~ Year) + theme_bw(base_size = 15) + theme(axis.text = element_text(size=8)) + scale_fill_gradientn(colours=matlab.like(15), labels = scales::percent, limits=rng) + theme(axis.text.x = element_text(angle = 75, hjust = 1, vjust = 1)) + labs(fill = &quot;Change in\\nSafe Winter Chill\\nsince 1980&quot;) + scale_y_discrete(labels=model_labels) + ylab(&quot;Chill metric&quot;) We do a similar procedure for the past data by filtering it out with GCM==\"none\". p_past&lt;- ggplot(results[which(results$GCM==&quot;none&quot;),], aes(Year, y=factor(Metric, levels=metrics), fill = SWC)) + geom_tile() p_past&lt;- p_past + theme_bw(base_size = 15) + theme(axis.text = element_text(size=8)) + scale_fill_gradientn(colours=matlab.like(15), labels = scales::percent, limits=rng) + scale_x_continuous(position = &quot;top&quot;) + labs(fill = &quot;Change in\\nSafe Winter Chill\\nsince 1980&quot;) + scale_y_discrete(labels=model_labels) + ylab(&quot;Chill metric&quot;) With the patchwork package, we can combine the previously created plots into one graph. With nrow we can determine the formation and with ´guides = “collect”´ we create only one legend for both plots. chill_comp_plot &lt;- (p_past + p_future + plot_layout(guides = &quot;collect&quot;,nrow=2, heights=c(1,2))) &amp; theme(plot.margin = margin(1, 1, 12, 1), legend.position = &quot;right&quot;, strip.background = element_blank(), strip.text = element_text(face = &quot;bold&quot;)) chill_comp_plot Figure 16.1: Heatmap for change in SWC for different chill metrics for the past and four future scenarios The plot shows the Safe Winter chill depending on different scenarios from the past and for the future. The best we can see in this plot is that the North Carolina Chill model should not be applied in South Africa. The model goes completely crazy and predicts changes of several thousand percent. This causes the problem that the scaling is also increasing and the predicted changes from the other models do not show up well. I made another plot below where I this time filtered out the North Carolina model. A few models still show some peculiarities, but overall we can now see the results of the other models more clearly. The predictions of the models generally indicate a decrease in available chill. results_wo_North &lt;- filter(results, !Metric==&quot;North_Carolina&quot;) rng = range(results_wo_North$SWC) p_future2 &lt;- ggplot(results_wo_North[which(!results_wo_North$GCM==&quot;none&quot;),], aes(GCM, y=factor(Metric, levels=metrics[metrics!= &quot;North_Carolina&quot;]), fill = SWC)) + geom_tile() + facet_grid(RCP ~ Year) + theme_bw(base_size = 15) + theme(axis.text = element_text(size=8)) + scale_fill_gradientn(colours=matlab.like(15), labels = scales::percent, limits=rng) + theme(axis.text.x = element_text(angle = 75, hjust = 1, vjust = 1)) + labs(fill = &quot;Change in\\nSafe Winter Chill\\nsince 1980&quot;) + scale_y_discrete(labels=model_labels[model_labels!= &quot;North Carolina Chill Units&quot;]) + ylab(&quot;Chill metric&quot;) p2_past &lt;- ggplot(results_wo_North[which(results_wo_North$GCM==&quot;none&quot;),], aes(Year, y=factor(Metric, levels=metrics[metrics!= &quot;North_Carolina&quot;]), fill = SWC)) + geom_tile() p2_past&lt;- p2_past + theme_bw(base_size = 15) + theme(axis.text = element_text(size=8)) + scale_fill_gradientn(colours=matlab.like(15), labels = scales::percent, limits=rng) + scale_x_continuous(position = &quot;top&quot;) + labs(fill = &quot;Change in\\nSafe Winter Chill\\nsince 1980&quot;) + scale_y_discrete(labels=model_labels[model_labels!= &quot;North Carolina Chill Units&quot;]) + ylab(&quot;Chill metric&quot;) chill_comp_plot2 &lt;- (p2_past + p_future2 + plot_layout(guides = &quot;collect&quot;,nrow=2, heights=c(1,2))) &amp; theme(plot.margin = margin(1, 1, 12, 1), legend.position = &quot;right&quot;, strip.background = element_blank(), strip.text = element_text(face = &quot;bold&quot;)) Show/Hide code Figure 16.2: Adjusted heatmap for change in SWC for different chill metrics for the past and four future scenarios 16.3 Produce an animated line plot of your results (summarizing Safe Winter Chill across all the GCMs). The animated plot is based on a line plot, that was animated with transition_reveal(Year) from the gganimate package. The line plot is revealed in order of the ´Year´ variable. Instead of plotting the graph directly in the markdown document, I decided to save it as a file and import it. This allowed me to knit the document faster. hist_results&lt;-results[which(results$GCM==&quot;none&quot;),] hist_results$RCP&lt;-&quot;RCP4.5&quot; hist_results_2&lt;-hist_results hist_results_2$RCP&lt;-&quot;RCP8.5&quot; hist_results&lt;-rbind(hist_results,hist_results_2) future_results&lt;-results[which(!results$GCM==&quot;none&quot;),] GCM_aggregate&lt;-aggregate( future_results$SWC, by=list(future_results$Metric,future_results$RCP,future_results$Year), FUN=mean) colnames(GCM_aggregate)&lt;-c(&quot;Metric&quot;,&quot;RCP&quot;,&quot;Year&quot;,&quot;SWC&quot;) RCP_Time_series&lt;-rbind(hist_results[,c(&quot;Metric&quot;,&quot;RCP&quot;,&quot;Year&quot;,&quot;SWC&quot;)], GCM_aggregate) chill_change_plot &lt;- ggplot(data=RCP_Time_series, aes(x=Year,y=SWC,col=factor(Metric,levels=metrics))) + geom_line(lwd=1.3) + facet_wrap(~RCP,nrow=2) + theme_bw(base_size=15) + labs(col = &quot;Change in\\nSafe Winter Chill\\nsince 1980&quot;) + scale_color_discrete(labels=model_labels) + scale_y_continuous(labels = scales::percent) + theme(strip.background = element_blank(), strip.text = element_text(face = &quot;bold&quot;)) + ylab(&quot;Safe Winter Chill&quot;) library(gganimate) library(gifski) myPlot &lt;- chill_change_plot + transition_reveal(Year) animate(myPlot, renderer = gifski_renderer()) anim_save(&quot;plots/chill_comparison_animation.gif&quot;, animation = last_animation()) The line plot also shows again clearly that the North Carolina Chill model is not suitable for South Africa. For the other models, we can see that the SWC will decrease. Figure 16.3: Animated line plot for the change in SWC depending on different chill models for Cape Town, South Africa since 1980 "],["chill-model-comparison-1.html", "Chapter 17 Chill model comparison 17.1 Provide a brief narrative describing what p-hacking is, and why this is a problematic approach to data analysis. 17.2 Provide a sketch of your causal understanding of the relationship between temperature and bloom dates. 17.3 What do we need to know to build a process-based model from this?", " Chapter 17 Chill model comparison 17.1 Provide a brief narrative describing what p-hacking is, and why this is a problematic approach to data analysis. P-hacking is the term used when scientists artificially represent their results in a significant way. Large data sets always contain significant structures. By data analysis and there especially often by machine learning methods such correlations are specifically searched for, although they have no relevance for the actual application. Therefore, it is important to understand the data in the context of its origin and its natural process. Another form of p-hacking can be seen in the cartoon below, where 21 trials are made to achieve one significant result with a p-value below 0.05. This result is then broadly proclaimed, while the 20 other unsignificant trials are not mentioned Figure 17.1: “So, uh, we did the green study again and got no link. It was probably a–”RESEARCH CONFLICTED ON GREEN JELLY BEAN/ACNE LINK; MORE STUDY RECOMMENDED!” Source: xkcd 17.2 Provide a sketch of your causal understanding of the relationship between temperature and bloom dates. Temperature is an important factor regarding the development of temperate fruit trees during the winter. During the winter these trees enter the state of dormancy, which ceases with the combination of the right temperatures. During this time, the tree appears inactive from the outside, although various processes are taking place inside. Those processes are driven by the main two phases of dormancy, the endodormancy and the ecodormancy, whose requirements are fulfilled by specific amounts of chill and heat. Their fulfillment results in the overcoming of dormancy and the consequent continuation of development. This allows the buds to break in the spring before the flower begins to bloom thereafter. Different temperatures during dormancy thus lead to a delay or advance of flowering. I plotted a very simplified sketch with a function from the DiagrammeR package. Some points of the graph could be improved, but to show that the temperature does not directly affect the Bloom date it is enough. require(DiagrammeR) plot &lt;- grViz(&quot;digraph dot { graph [layout = dot rankdir = LR fontsize = &#39;12&#39; fontname = Arial compound = true newrank=true] node [shape = rectangle, style = filled, fillcolor = aliceblue, fontname = Arial, fontsize = &#39;12&#39;] a [label = &#39;Temperature&#39;] d [label = &#39;Phenology&#39;] e [label = &#39;Cultivar specific&#39;] f [label = &#39;Bloom Dates&#39;] g [label = &#39;Climate forcing&#39;, fillcolor = gray98, fontcolor = gray60, color = gray60] h [label = &#39;Greenhouse Gas Emissions&#39;, fillcolor = gray98, fontcolor = gray60, color = gray60] i [label = &#39;Geographical location&#39;, fillcolor = gray98, fontcolor = gray60, color = gray60] l [style=invis] subgraph cluster { label=&#39;Dormancy&#39; fontsize = &#39;14&#39; fillcolor = &#39;lightskyblue1:tomato&#39; style = filled gradientangle = 0 b [label = &#39;Chill requirement&#39;, fillcolor = lightskyblue] c [label = &#39;Heat requirements&#39;, fillcolor = tomato] j [label = &#39;Chill&#39;, fillcolor = lightskyblue] k [label = &#39;Heat&#39;, fillcolor = tomato]} subgraph cluster2 { label = &#39;Internal processes&#39; ranksep = 2.2 fontsize = &#39;16&#39; fillcolor = &#39;grey90&#39; style = invis} a-&gt;j j-&gt;b a-&gt;k k-&gt;c [minlen = 2] b-&gt;d c-&gt;d [minlen = 2] e-&gt;b [lhead = cluster, minlen=3] d-&gt;f h-&gt;g g-&gt;a i-&gt;a [constraint=false] { rank = same; h; g; a; i} { rank = same; e; b; l } { rank = same; l; k; l }}&quot;) Show/Hide code Figure 17.2: Sketch for the relationship between temperature and bloom dates 17.3 What do we need to know to build a process-based model from this? To build a process-based model, we need to understand the relation between temperature and the bloom dates of the trees. We already know, that an important factor in this relation is the dormancy with its chill and heat related phases. For a process based model it is important which effect the temperatures have on the individual phases. For this purpose, there are already individual models such as the Dynamic Model for the chilling phase or the Growing Degree Hours model for the forcing period. These can be the basis for a process based model, and must then be combined reasonably to calculate a bloom date from a long time series of temperatures. "],["delineating-temperature-response-phases-with-pls-regression.html", "Chapter 18 Delineating temperature response phases with PLS regression 18.1 Briefly explain why you shouldn’t take the results of a PLS regression analysis between temperature and phenology at face value. What do you need in addition in order to make sense of such outputs? 18.2 Replicate the PLS analysis for the Roter Boskoop dataset that you used in a previous lesson. 18.3 Write down your thoughts on why we’re not seeing the temperature response pattern we may have expected. What happened to the chill response?", " Chapter 18 Delineating temperature response phases with PLS regression 18.1 Briefly explain why you shouldn’t take the results of a PLS regression analysis between temperature and phenology at face value. What do you need in addition in order to make sense of such outputs? The phenology of trees is determined by more than just the factor of temperature. Nevertheless, we use temperature as the only input because it is easier to handle and sufficient for our purpose. PLS is a method that is also used in machine learning. Without the necessary knowledge about the application behavior or without the right expectations, such a method can quickly lead to p-hacking or detect structures that are of no real importance. We need clear expectations on the results, like in our case the detection of the chilling and forcing phase to use PLS in a meaningful way. Furthermore, detected structures should not be overestimated. 18.2 Replicate the PLS analysis for the Roter Boskoop dataset that you used in a previous lesson. library(chillR) Boskoop&lt;-read_tab(&quot;data/Roter_Boskoop_bloom_1958_2019.csv&quot;) Boskoop_first&lt;-Boskoop[,1:2] Boskoop_first[,&quot;Year&quot;]&lt;-substr(Boskoop_first$First_bloom,1,4) Boskoop_first[,&quot;Month&quot;]&lt;-substr(Boskoop_first$First_bloom,5,6) Boskoop_first[,&quot;Day&quot;]&lt;-substr(Boskoop_first$First_bloom,7,8) Boskoop_first&lt;-make_JDay(Boskoop_first) Boskoop_first&lt;-Boskoop_first[,c(&quot;Pheno_year&quot;,&quot;JDay&quot;)] colnames(Boskoop_first)&lt;-c(&quot;Year&quot;,&quot;pheno&quot;) KA_temps&lt;-read_tab(&quot;data/TMaxTMin1958-2019_patched.csv&quot;) KA_temps&lt;-make_JDay(KA_temps) PLS_results&lt;-PLS_pheno(KA_temps,Boskoop_first) library(ggplot2) PLS_gg&lt;-PLS_results$PLS_summary PLS_gg[,&quot;Month&quot;]&lt;-trunc(PLS_gg$Date/100) PLS_gg[,&quot;Day&quot;]&lt;-PLS_gg$Date-PLS_gg$Month*100 PLS_gg[,&quot;Date&quot;]&lt;-ISOdate(2002,PLS_gg$Month,PLS_gg$Day) PLS_gg[which(PLS_gg$JDay&lt;=0),&quot;Date&quot;]&lt;- ISOdate(2001, PLS_gg$Month[which(PLS_gg$JDay&lt;=0)], PLS_gg$Day[which(PLS_gg$JDay&lt;=0)]) PLS_gg[,&quot;VIP_importance&quot;]&lt;-PLS_gg$VIP&gt;=0.8 PLS_gg[,&quot;VIP_Coeff&quot;]&lt;-factor(sign(PLS_gg$Coef)*PLS_gg$VIP_importance) VIP_plot&lt;- ggplot(PLS_gg,aes(x=Date,y=VIP)) + geom_bar(stat=&#39;identity&#39;,aes(fill=VIP&gt;0.8)) VIP_plot &lt;- VIP_plot + scale_fill_manual(name=&quot;VIP&quot;, labels = c(&quot;&lt;0.8&quot;, &quot;&gt;0.8&quot;), values = c(&quot;FALSE&quot;=&quot;grey&quot;, &quot;TRUE&quot;=&quot;blue&quot;)) + theme_bw(base_size=15) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank() ) coeff_plot&lt;- ggplot(PLS_gg,aes(x=Date,y=Coef)) + geom_bar(stat=&#39;identity&#39;,aes(fill=VIP_Coeff)) + scale_fill_manual(name=&quot;Effect direction&quot;, labels = c(&quot;Advancing&quot;, &quot;Unimportant&quot;,&quot;Delaying&quot;), values = c(&quot;-1&quot;=&quot;red&quot;, &quot;0&quot;=&quot;grey&quot;,&quot;1&quot;=&quot;dark green&quot;)) + theme_bw(base_size=15) + ylab(&quot;PLS coefficient&quot;) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank() ) temp_plot&lt;- ggplot(PLS_gg) + geom_ribbon(aes(x=Date,ymin=Tmean-Tstdev,ymax=Tmean+Tstdev), fill=&quot;grey&quot;) + geom_ribbon(aes(x=Date,ymin=Tmean-Tstdev*(VIP_Coeff==-1), ymax=Tmean+Tstdev*(VIP_Coeff==-1)), fill=&quot;red&quot;) + geom_ribbon(aes(x=Date,ymin=Tmean-Tstdev*(VIP_Coeff==1), ymax=Tmean+Tstdev*(VIP_Coeff==1)), fill=&quot;dark green&quot;) + geom_line(aes(x=Date,y=Tmean)) + theme_bw(base_size=15) + ylab(expression(paste(T[mean],&quot; (°C)&quot;))) library(patchwork) library(plotly) plot&lt;- (VIP_plot + coeff_plot + temp_plot + plot_layout(ncol=1, guides = &quot;collect&quot;) ) &amp; theme(legend.position = &quot;right&quot;, legend.text = element_text(size=8), legend.title = element_text(size=10), axis.title.x=element_blank()) Figure 18.1: Temperature response pattern of ‘Roter Boskoop’ apples at CKA based on a PLS regression. 18.3 Write down your thoughts on why we’re not seeing the temperature response pattern we may have expected. What happened to the chill response? In countries like Germany, the forcing period could be the stronger driver towards the flower development. It is also possible that PLS cannot detect the chill response, when there is no monotonic relationship between temperature and chill accumulation. A successful PLS regression depends on sufficient amounts of data. If the analysis is performed with not enough data, this can also lead to errors. However, this will probably not be the case in this example. "],["successes-and-limitations-of-pls-regression-analysis.html", "Chapter 19 Successes and limitations of PLS regression analysis 19.1 Briefly explain in what climatic settings we can expect PLS regression to detect the chilling phase - and in what settings this probably won’t work. 19.2 How could we overcome this problem?", " Chapter 19 Successes and limitations of PLS regression analysis 19.1 Briefly explain in what climatic settings we can expect PLS regression to detect the chilling phase - and in what settings this probably won’t work. We can expect PLS regression to detect the chilling phase, when the setting features a continuous monotonic relationship in temperature and generation of chill. In climate settings such as China or Germany a change in temperature can lead to more or less chill dependent on the initial temperature before the change. An increase in temperature from -5°C to 5°C leads to an increase in chill accumulation, while the same increase in the same direction from 5°C to 15°C leads to a decrease in chill accumulation. In California for example PLS detects the chilling phase, because chill accumulation monotonically declines with warmer temperatures or increases with colder temperatures. A decrease in chill due to too cold temperatures is less common. In the graph below I have added the temperatures for Tunisia. Robin kindly provided me with his temperature data from there. In Tunisia, as already seen in California, there is a monotonic relationship between temperature and chill accumulation. Thus, a PLS regression can be successfully performed there as well, as we will later see in Chapter 21. library(chillR) library(dormancyR) library(ggplot2) library(reshape2) library(patchwork) hourly_models &lt;- list(Chilling_units = chilling_units, Low_chill = low_chill_model, Modified_Utah = modified_utah_model, North_Carolina = north_carolina_model, Positive_Utah = positive_utah_model, Chilling_Hours = Chilling_Hours, Utah_Chill_Units = Utah_Model, Chill_Portions = Dynamic_Model) daily_models&lt;-list(Rate_of_Chill = rate_of_chill, Exponential_Chill = exponential_chill, Triangula_Chill_Haninnen = triangular_chill_1, Triangular_Chill_Legave = triangular_chill_2) metrics&lt;-c(names(daily_models),names(hourly_models)) model_labels=c(&quot;Rate of Chill&quot;, &quot;Exponential Chill&quot;, &quot;Triangular Chill (Häninnen)&quot;, &quot;Triangular Chill (Legave)&quot;, &quot;Chilling Units&quot;, &quot;Low-Chill Chill Units&quot;, &quot;Modified Utah Chill Units&quot;, &quot;North Carolina Chill Units&quot;, &quot;Positive Utah Chill Units&quot;, &quot;Chilling Hours&quot;, &quot;Utah Chill Units&quot;, &quot;Chill Portions&quot;) for(T in -20:30) {hourly&lt;-sapply(hourly_models, function(x) x(rep(T,1000)))[1000,] temp_frame&lt;-data.frame(Tmin=rep(T,1000), Tmax=rep(T,1000), Tmean=rep(T,1000)) daily&lt;-sapply(daily_models, function(x) x(temp_frame))[1000,] if(T==-20) sensitivity&lt;-c(T=T,daily,hourly) else sensitivity&lt;-rbind(sensitivity,c(T=T,daily,hourly)) } sensitivity_normal&lt;- as.data.frame(cbind(sensitivity[,1], sapply(2:ncol(sensitivity), function(x) sensitivity[,x]/max(sensitivity[,x])))) colnames(sensitivity_normal)&lt;-colnames(sensitivity) sensitivity_gg&lt;-melt(sensitivity_normal,id.vars=&quot;T&quot;) sensitivity_gg$value[which(sensitivity_gg$value&lt;=0.001)]&lt;-NA chill&lt;- ggplot(sensitivity_gg,aes(x=T,y=factor(variable),size=value)) + geom_point(col=&quot;light blue&quot;) + scale_y_discrete(labels= model_labels) + ylab(&quot;Chill model&quot;) + xlab(&quot;Temperature (assumed constant, °C)&quot;) + xlim(c(-30,40)) + theme_bw(base_size=15) + labs(size = &quot;Chill \\nWeight&quot;) # Temperature data KA_temps_JD&lt;-make_JDay(read_tab(&quot;data/TMaxTMin1958-2019_patched.csv&quot;)) temps&lt;-stack_hourly_temps( KA_temps_JD[which(KA_temps_JD$JDay&gt;305|KA_temps_JD$JDay&lt;90),], latitude=50.6) hh&lt;-hist(temps$hourtemps$Temp,breaks=c(-30:30), plot=FALSE) hh_df&lt;-data.frame( T=hh$mids, variable=&quot;Klein-Altendorf, Germany&quot;, value=hh$counts/max(hh$counts)) hh_df$value[which(hh_df$value==0)]&lt;-NA Beijing_temps_JD&lt;-make_JDay(read_tab(&quot;data/Beijing_weather.csv&quot;)) temps&lt;-stack_hourly_temps( Beijing_temps_JD[which(Beijing_temps_JD$JDay&gt;305|Beijing_temps_JD$JDay&lt;90),] ,latitude=39.9) hh&lt;-hist(temps$hourtemps$Temp,breaks=c(-30:30), plot=FALSE) hh_df_2&lt;-data.frame(T=hh$mids, variable=&quot;Beijing, China&quot;,value=hh$counts/max(hh$counts)) hh_df_2$value[which(hh_df_2$value==0)]&lt;-NA Davis_temps_JD&lt;-make_JDay(read_tab(&quot;data/Davis_weather.csv&quot;)) temps&lt;-stack_hourly_temps( Davis_temps_JD[which(Davis_temps_JD$JDay&gt;305|Davis_temps_JD$JDay&lt;90),], latitude=38.5) hh&lt;-hist(temps$hourtemps$Temp,breaks=c(-30:40), plot=FALSE) hh_df_3&lt;-data.frame(T=hh$mids, variable=&quot;Davis, California&quot;, value=hh$counts/max(hh$counts)) hh_df_3$value[which(hh_df_3$value==0)]&lt;-NA # added coords &lt;- c(18.8148, -33.9575) CT_temps_JD&lt;-make_JDay(read_tab(&quot;data/CapeTown_chillR_weather2.csv&quot;)) temps&lt;-stack_hourly_temps( CT_temps_JD[which(CT_temps_JD$JDay&gt;121|CT_temps_JD$JDay&lt;273),], latitude=coords[2]) hh&lt;-hist(temps$hourtemps$Temp,breaks=c(-30:50), plot=FALSE) hh_df_4&lt;-data.frame( T=hh$mids, variable=&quot;Cape Town, South Africa&quot;, value=hh$counts/max(hh$counts)) hh_df_4$value[which(hh_df_4$value==0)]&lt;-NA # added TT_temps_JD&lt;-make_JDay(read_tab(&quot;data/Tunis_weather_fixed.csv&quot;)) temps&lt;-stack_hourly_temps( TT_temps_JD[which(TT_temps_JD$JDay&gt;305|TT_temps_JD$JDay&lt;90),], latitude=36.78) hh&lt;-hist(temps$hourtemps$Temp,breaks=c(-30:50), plot=FALSE) hh_df_5&lt;-data.frame( T=hh$mids, variable=&quot;Tunis, Tunisia&quot;, value=hh$counts/max(hh$counts)) hh_df_5$value[which(hh_df_5$value==0)]&lt;-NA hh_df&lt;-rbind(hh_df,hh_df_2,hh_df_3,hh_df_5) # not plotting South Africa, because location on southern hemisphere # plot for the locations locations&lt;- ggplot(data=hh_df,aes(x=T,y=variable,size=value)) + geom_point(col=&quot;coral2&quot;) + ylab(&quot;Location&quot;) + xlab(&quot;Temperature (between November and March, °C)&quot;) + xlim(c(-30,40)) + theme_bw(base_size=15) + labs(size = &quot;Relative \\nfrequency&quot;) # plot for the Dynamic model sensitivity chill&lt;- ggplot(sensitivity_gg[which(sensitivity_gg$variable==&quot;Chill_Portions&quot;),], aes(x=T,y=factor(variable), size=value)) + geom_point(col=&quot;light blue&quot;) + scale_y_discrete(labels= &quot;Chill Portions&quot;) + ylab(&quot;Chill model&quot;) + xlab(&quot;Temperature (assumed constant, °C)&quot;) + xlim(c(-30,40)) + theme_bw(base_size=15) + labs(size = &quot;Chill \\nWeight&quot;) plot&lt;- (chill + locations + plot_layout(guides = &quot;collect&quot;, heights = c(0.5,1)) ) &amp; theme(legend.position = &quot;right&quot;, legend.text = element_text(size=10), legend.title = element_text(size=12)) Show/Hide code Figure 19.1: Comparison between model sensitivity and temperatures prevailing at the locations 19.2 How could we overcome this problem? To overcome this problem, we should not perform PLS analysis on temperature but directly from the chill accumulation. This filters out the problems caused by the double-sided effect at cool temperatures. Because the chill models are not exactly accurate, we introduce a certain error into our data, which would not have been present with the temperature alone. On the other hand, this allows us to delineated the phases, especially the chilling phase, easier. "],["pls-regression-with-agroclimatic-metrics.html", "Chapter 20 PLS regression with agroclimatic metrics 20.1 Repeat the PLS_chill_force procedure for the ‘Roter Boskoop’ dataset. Include plots of daily chill and heat accumulation. 20.2 Run PLS_chill_force analyses for all three major chill models. Delineate your best estimates of chilling and forcing phases for all of them. 20.3 Plot results for all three analyses, including shaded plot areas for the chilling and forcing periods you estimated.", " Chapter 20 PLS regression with agroclimatic metrics 20.1 Repeat the PLS_chill_force procedure for the ‘Roter Boskoop’ dataset. Include plots of daily chill and heat accumulation. library(chillR) temps&lt;-read_tab(&quot;data/TMaxTMin1958-2019_patched.csv&quot;) temps_hourly&lt;-stack_hourly_temps(temps,latitude=50.6) daychill&lt;-daily_chill(hourtemps=temps_hourly, running_mean=1, models = list(Chilling_Hours = Chilling_Hours, Utah_Chill_Units = Utah_Model, Chill_Portions = Dynamic_Model, GDH = GDH)) Boskoop &lt;- read_tab(&quot;data/Roter_Boskoop_bloom_1958_2019.csv&quot;) Boskoop_first &lt;- Boskoop[,1:2] Boskoop_first[,&quot;Year&quot;] &lt;- substr(Boskoop_first$First_bloom,1,4) Boskoop_first[,&quot;Month&quot;] &lt;- substr(Boskoop_first$First_bloom,5,6) Boskoop_first[,&quot;Day&quot;] &lt;- substr(Boskoop_first$First_bloom,7,8) Boskoop_first &lt;- make_JDay(Boskoop_first) Boskoop_first &lt;- Boskoop_first[,c(&quot;Pheno_year&quot;,&quot;JDay&quot;)] colnames(Boskoop_first) &lt;- c(&quot;Year&quot;,&quot;pheno&quot;) plscf &lt;- PLS_chill_force(daily_chill_obj=daychill, bio_data_frame=Boskoop_first, split_month=6, chill_models = &quot;Chill_Portions&quot;, heat_models = &quot;GDH&quot;, runn_means = 11) Table 20.1: Extract of the coefficients from the PLS analysis Date Type JDay Coef VIP MetricMean MetricStdev 701 Chill -183 0.1137146 1.061441 0.0559804 0.0965426 702 Chill -182 0.1330927 1.182803 0.0563105 0.0997921 703 Chill -181 0.1127799 1.077426 0.0532572 0.0956684 704 Chill -180 0.1739338 1.333523 0.0486906 0.0866341 705 Chill -179 0.0967065 1.121475 0.0425852 0.0815729 706 Chill -178 0.0696730 1.085244 0.0397766 0.0802201 PLS_gg&lt;-plscf$Chill_Portions$GDH$PLS_summary PLS_gg[,&quot;Month&quot;] &lt;- trunc(PLS_gg$Date/100) PLS_gg[,&quot;Day&quot;] &lt;- PLS_gg$Date-PLS_gg$Month*100 PLS_gg[,&quot;Date&quot;] &lt;- ISOdate(2002,PLS_gg$Month,PLS_gg$Day) PLS_gg[which(PLS_gg$JDay&lt;=0),&quot;Date&quot;] &lt;- ISOdate(2001, PLS_gg$Month[which(PLS_gg$JDay&lt;=0)], PLS_gg$Day[which(PLS_gg$JDay&lt;=0)]) PLS_gg[,&quot;VIP_importance&quot;] &lt;- PLS_gg$VIP&gt;=0.8 PLS_gg[,&quot;VIP_Coeff&quot;] &lt;- factor(sign(PLS_gg$Coef)*PLS_gg$VIP_importance) chill_start_JDay&lt;--48 #! chill_end_JDay&lt;-72 heat_start_JDay&lt;--8 heat_end_JDay&lt;-110 chill_start_date&lt;-ISOdate(2001,12,31)+chill_start_JDay*24*3600 chill_end_date&lt;-ISOdate(2001,12,31)+chill_end_JDay*24*3600 heat_start_date&lt;-ISOdate(2001,12,31)+heat_start_JDay*24*3600 heat_end_date&lt;-ISOdate(2001,12,31)+heat_end_JDay*24*3600 library(ggplot2) temp_plot&lt;- ggplot(PLS_gg,x=Date) + annotate(&quot;rect&quot;, xmin = chill_start_date, xmax = chill_end_date, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;blue&quot;) + annotate(&quot;rect&quot;, xmin = heat_start_date, xmax = heat_end_date, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;red&quot;) + annotate(&quot;rect&quot;, xmin = ISOdate(2001,12,31) + min(plscf$pheno$pheno,na.rm=TRUE)*24*3600, xmax = ISOdate(2001,12,31) + max(plscf$pheno$pheno,na.rm=TRUE)*24*3600, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;black&quot;) + geom_vline(xintercept = ISOdate(2001,12,31) + median(plscf$pheno$pheno,na.rm=TRUE)*24*3600, linetype = &quot;dashed&quot;) + geom_ribbon(aes(x=Date, ymin=MetricMean - MetricStdev , ymax=MetricMean + MetricStdev ), fill=&quot;grey&quot;) + geom_ribbon(aes(x=Date, ymin=MetricMean - MetricStdev * (VIP_Coeff==-1), ymax=MetricMean + MetricStdev * (VIP_Coeff==-1)), fill=&quot;red&quot;) + geom_ribbon(aes(x=Date, ymin=MetricMean - MetricStdev * (VIP_Coeff==1), ymax=MetricMean + MetricStdev * (VIP_Coeff==1)), fill=&quot;dark green&quot;) + geom_line(aes(x=Date,y=MetricMean )) temp_plot&lt;- temp_plot + facet_wrap(vars(Type), scales = &quot;free_y&quot;, strip.position=&quot;left&quot;, labeller = labeller(Type = as_labeller( c(Chill=&quot;Chill (CP)&quot;,Heat=&quot;Heat (GDH)&quot;)))) + ggtitle(&quot;Daily chill and heat accumulation rates&quot;) + theme_bw(base_size=15) + theme(strip.background = element_blank(), strip.placement = &quot;outside&quot;, strip.text.y = element_text(size =12), plot.title = element_text(hjust = 0.5), axis.title.y=element_blank() ) Figure 20.1: Daily chill and heat accumulation rates plot as part for the final graph VIP_plot&lt;- ggplot(PLS_gg,aes(x=Date,y=VIP)) + annotate(&quot;rect&quot;, xmin = chill_start_date, xmax = chill_end_date, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;blue&quot;) + annotate(&quot;rect&quot;, xmin = heat_start_date, xmax = heat_end_date, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;red&quot;) + annotate(&quot;rect&quot;, xmin = ISOdate(2001,12,31) + min(plscf$pheno$pheno,na.rm=TRUE)*24*3600, xmax = ISOdate(2001,12,31) + max(plscf$pheno$pheno,na.rm=TRUE)*24*3600, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;black&quot;) + geom_vline(xintercept = ISOdate(2001,12,31) + median(plscf$pheno$pheno,na.rm=TRUE)*24*3600, linetype = &quot;dashed&quot;) + geom_bar(stat=&#39;identity&#39;,aes(fill=VIP&gt;0.8)) VIP_plot &lt;- VIP_plot + facet_wrap(vars(Type), scales=&quot;free&quot;, strip.position=&quot;left&quot;, labeller = labeller(Type = as_labeller( c(Chill=&quot;VIP for chill&quot;,Heat=&quot;VIP for heat&quot;)))) + scale_y_continuous( limits=c(0,max(plscf$Chill_Portions$GDH$PLS_summary$VIP))) + ggtitle(&quot;Variable Importance in the Projection (VIP) scores&quot;) + theme_bw(base_size=15) + theme(strip.background = element_blank(), strip.placement = &quot;outside&quot;, strip.text.y = element_text(size =12), plot.title = element_text(hjust = 0.5), axis.title.y=element_blank() ) VIP_plot &lt;- VIP_plot + scale_fill_manual(name=&quot;VIP&quot;, labels = c(&quot;&lt;0.8&quot;, &quot;&gt;0.8&quot;), values = c(&quot;FALSE&quot;=&quot;grey&quot;, &quot;TRUE&quot;=&quot;blue&quot;)) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank()) Figure 20.2: VIP scores plotted and used as part for the final graph coeff_plot&lt;- ggplot(PLS_gg,aes(x=Date,y=Coef)) + annotate(&quot;rect&quot;, xmin = chill_start_date, xmax = chill_end_date, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;blue&quot;) + annotate(&quot;rect&quot;, xmin = heat_start_date, xmax = heat_end_date, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;red&quot;) + annotate(&quot;rect&quot;, xmin = ISOdate(2001,12,31) + min(plscf$pheno$pheno,na.rm=TRUE)*24*3600, xmax = ISOdate(2001,12,31) + max(plscf$pheno$pheno,na.rm=TRUE)*24*3600, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;black&quot;) + geom_vline(xintercept = ISOdate(2001,12,31) + median(plscf$pheno$pheno,na.rm=TRUE)*24*3600, linetype = &quot;dashed&quot;) + geom_bar(stat=&#39;identity&#39;,aes(fill=VIP_Coeff)) coeff_plot &lt;- coeff_plot + facet_wrap(vars(Type), scales=&quot;free&quot;, strip.position=&quot;left&quot;, labeller = labeller( Type = as_labeller( c(Chill=&quot;MC for chill&quot;,Heat=&quot;MC for heat&quot;)))) + scale_y_continuous( limits=c(min(plscf$Chill_Portions$GDH$PLS_summary$Coef), max(plscf$Chill_Portions$GDH$PLS_summary$Coef))) + ggtitle(&quot;Model coefficients (MC)&quot;) + theme_bw(base_size=15) + theme(strip.background = element_blank(), strip.placement = &quot;outside&quot;, strip.text.y = element_text(size =12), plot.title = element_text(hjust = 0.5), axis.title.y=element_blank() ) coeff_plot &lt;- coeff_plot + facet_wrap(vars(Type), scales=&quot;free&quot;, strip.position=&quot;left&quot;, labeller = labeller( Type = as_labeller( c(Chill=&quot;MC for chill&quot;,Heat=&quot;MC for heat&quot;)))) + scale_y_continuous( limits=c(min(plscf$Chill_Portions$GDH$PLS_summary$Coef), max(plscf$Chill_Portions$GDH$PLS_summary$Coef))) + ggtitle(&quot;Model coefficients (MC)&quot;) + theme_bw(base_size=15) + theme(strip.background = element_blank(), strip.placement = &quot;outside&quot;, strip.text.y = element_text(size =12), plot.title = element_text(hjust = 0.5), axis.title.y=element_blank() ) coeff_plot &lt;- coeff_plot + scale_fill_manual(name=&quot;Effect direction&quot;, labels = c(&quot;Advancing&quot;, &quot;Unimportant&quot;,&quot;Delaying&quot;), values = c(&quot;-1&quot;=&quot;red&quot;, &quot;0&quot;=&quot;grey&quot;,&quot;1&quot;=&quot;dark green&quot;)) + ylab(&quot;PLS coefficient&quot;) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank()) Figure 20.3: Model coefficients plotted for the use in the final graph library(patchwork) plot&lt;- (VIP_plot + coeff_plot + temp_plot + plot_layout(ncol=1, guides = &quot;collect&quot;) ) &amp; theme(legend.position = &quot;right&quot;, legend.text = element_text(size=8), legend.title = element_text(size=10), axis.title.x=element_blank()) Figure 20.4: Final result with all three subplots patched together I hid the code for the plot_PLS_chill_force() function behind the button below to save some space. plot_PLS_chill_force&lt;-function(plscf, chill_metric=&quot;Chill_Portions&quot;, heat_metric=&quot;GDH&quot;, chill_label=&quot;CP&quot;, heat_label=&quot;GDH&quot;, chill_phase=c(-48,62), heat_phase=c(-5,105.5), plot_title=&quot; &quot;) { PLS_gg&lt;-plscf[[chill_metric]][[heat_metric]]$PLS_summary PLS_gg[,&quot;Month&quot;]&lt;-trunc(PLS_gg$Date/100) PLS_gg[,&quot;Day&quot;]&lt;-PLS_gg$Date-PLS_gg$Month*100 PLS_gg[,&quot;Date&quot;]&lt;-ISOdate(2002,PLS_gg$Month,PLS_gg$Day) PLS_gg[which(PLS_gg$JDay&lt;=0),&quot;Date&quot;]&lt;-ISOdate(2001,PLS_gg$Month[which(PLS_gg$JDay&lt;=0)],PLS_gg$Day[which(PLS_gg$JDay&lt;=0)]) PLS_gg[,&quot;VIP_importance&quot;]&lt;-PLS_gg$VIP&gt;=0.8 PLS_gg[,&quot;VIP_Coeff&quot;]&lt;-factor(sign(PLS_gg$Coef)*PLS_gg$VIP_importance) chill_start_date&lt;-ISOdate(2001,12,31)+chill_phase[1]*24*3600 chill_end_date&lt;-ISOdate(2001,12,31)+chill_phase[2]*24*3600 heat_start_date&lt;-ISOdate(2001,12,31)+heat_phase[1]*24*3600 heat_end_date&lt;-ISOdate(2001,12,31)+heat_phase[2]*24*3600 temp_plot&lt;- ggplot(PLS_gg) + annotate(&quot;rect&quot;, xmin = chill_start_date, xmax = chill_end_date, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;blue&quot;) + annotate(&quot;rect&quot;, xmin = heat_start_date, xmax = heat_end_date, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;red&quot;) + annotate(&quot;rect&quot;, xmin = ISOdate(2001,12,31) + min(plscf$pheno$pheno,na.rm=TRUE)*24*3600, xmax = ISOdate(2001,12,31) + max(plscf$pheno$pheno,na.rm=TRUE)*24*3600, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;black&quot;) + geom_vline(xintercept = ISOdate(2001,12,31) + median(plscf$pheno$pheno,na.rm=TRUE)*24*3600, linetype = &quot;dashed&quot;) + geom_ribbon(aes(x=Date, ymin=MetricMean - MetricStdev , ymax=MetricMean + MetricStdev ), fill=&quot;grey&quot;) + geom_ribbon(aes(x=Date, ymin=MetricMean - MetricStdev * (VIP_Coeff==-1), ymax=MetricMean + MetricStdev * (VIP_Coeff==-1)), fill=&quot;red&quot;) + geom_ribbon(aes(x=Date, ymin=MetricMean - MetricStdev * (VIP_Coeff==1), ymax=MetricMean + MetricStdev * (VIP_Coeff==1)), fill=&quot;dark green&quot;) + geom_line(aes(x=Date,y=MetricMean )) + facet_wrap(vars(Type), scales = &quot;free_y&quot;, strip.position=&quot;left&quot;, labeller = labeller(Type = as_labeller(c(Chill=paste0(&quot;Chill (&quot;,chill_label,&quot;)&quot;),Heat=paste0(&quot;Heat (&quot;,heat_label,&quot;)&quot;)) )) ) + ggtitle(&quot;Daily chill and heat accumulation rates&quot;) + theme_bw(base_size=15) + theme(strip.background = element_blank(), strip.placement = &quot;outside&quot;, strip.text.y = element_text(size =12), plot.title = element_text(hjust = 0.5), axis.title.y=element_blank() ) VIP_plot&lt;- ggplot(PLS_gg,aes(x=Date,y=VIP)) + annotate(&quot;rect&quot;, xmin = chill_start_date, xmax = chill_end_date, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;blue&quot;) + annotate(&quot;rect&quot;, xmin = heat_start_date, xmax = heat_end_date, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;red&quot;) + annotate(&quot;rect&quot;, xmin = ISOdate(2001,12,31) + min(plscf$pheno$pheno,na.rm=TRUE)*24*3600, xmax = ISOdate(2001,12,31) + max(plscf$pheno$pheno,na.rm=TRUE)*24*3600, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;black&quot;) + geom_vline(xintercept = ISOdate(2001,12,31) + median(plscf$pheno$pheno,na.rm=TRUE)*24*3600, linetype = &quot;dashed&quot;) + geom_bar(stat=&#39;identity&#39;,aes(fill=VIP&gt;0.8)) + facet_wrap(vars(Type), scales=&quot;free&quot;, strip.position=&quot;left&quot;, labeller = labeller(Type = as_labeller(c(Chill=&quot;VIP for chill&quot;,Heat=&quot;VIP for heat&quot;) )) ) + scale_y_continuous(limits=c(0,max(plscf[[chill_metric]][[heat_metric]]$PLS_summary$VIP))) + ggtitle(&quot;Variable Importance in the Projection (VIP) scores&quot;) + theme_bw(base_size=15) + theme(strip.background = element_blank(), strip.placement = &quot;outside&quot;, strip.text.y = element_text(size =12), plot.title = element_text(hjust = 0.5), axis.title.y=element_blank() ) + scale_fill_manual(name=&quot;VIP&quot;, labels = c(&quot;&lt;0.8&quot;, &quot;&gt;0.8&quot;), values = c(&quot;FALSE&quot;=&quot;grey&quot;, &quot;TRUE&quot;=&quot;blue&quot;)) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank()) coeff_plot&lt;- ggplot(PLS_gg,aes(x=Date,y=Coef)) + annotate(&quot;rect&quot;, xmin = chill_start_date, xmax = chill_end_date, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;blue&quot;) + annotate(&quot;rect&quot;, xmin = heat_start_date, xmax = heat_end_date, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;red&quot;) + annotate(&quot;rect&quot;, xmin = ISOdate(2001,12,31) + min(plscf$pheno$pheno,na.rm=TRUE)*24*3600, xmax = ISOdate(2001,12,31) + max(plscf$pheno$pheno,na.rm=TRUE)*24*3600, ymin = -Inf, ymax = Inf, alpha = .1,fill = &quot;black&quot;) + geom_vline(xintercept = ISOdate(2001,12,31) + median(plscf$pheno$pheno,na.rm=TRUE)*24*3600, linetype = &quot;dashed&quot;) + geom_bar(stat=&#39;identity&#39;,aes(fill=VIP_Coeff)) + facet_wrap(vars(Type), scales=&quot;free&quot;, strip.position=&quot;left&quot;, labeller = labeller(Type = as_labeller(c(Chill=&quot;MC for chill&quot;,Heat=&quot;MC for heat&quot;) )) ) + scale_y_continuous(limits=c(min(plscf[[chill_metric]][[heat_metric]]$PLS_summary$Coef), max(plscf[[chill_metric]][[heat_metric]]$PLS_summary$Coef))) + ggtitle(&quot;Model coefficients (MC)&quot;) + theme_bw(base_size=15) + theme(strip.background = element_blank(), strip.placement = &quot;outside&quot;, strip.text.y = element_text(size =12), plot.title = element_text(hjust = 0.5), axis.title.y=element_blank() ) + scale_fill_manual(name=&quot;Effect direction&quot;, labels = c(&quot;Advancing&quot;, &quot;Unimportant&quot;,&quot;Delaying&quot;), values = c(&quot;-1&quot;=&quot;red&quot;, &quot;0&quot;=&quot;grey&quot;,&quot;1&quot;=&quot;dark green&quot;)) + ylab(&quot;PLS coefficient&quot;) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank()) library(patchwork) plot&lt;- (VIP_plot + coeff_plot + temp_plot + plot_layout(ncol=1, guides = &quot;collect&quot;)) + theme(legend.position = &quot;right&quot;, legend.text = element_text(size=8), legend.title = element_text(size=10), axis.title.x=element_blank()) + plot_annotation(title = plot_title) plot } Show/Hide code 20.2 Run PLS_chill_force analyses for all three major chill models. Delineate your best estimates of chilling and forcing phases for all of them. daychill &lt;- daily_chill(hourtemps=temps_hourly, running_mean=9, models = list(Chilling_Hours = Chilling_Hours, Utah_Chill_Units = Utah_Model, Chill_Portions = Dynamic_Model, GDH = GDH) ) plscf &lt;- PLS_chill_force(daily_chill_obj=daychill, bio_data_frame=Boskoop_first, split_month=6, chill_models = c(&quot;Chilling_Hours&quot;, &quot;Utah_Chill_Units&quot;, &quot;Chill_Portions&quot;), heat_models = c(&quot;GDH&quot;)) We can delineate the estimates of the chilling and forcing phases by having a direct look on the output of the PLS analysis another possibility is to estimate the phases with the help of the plotted version. options(max.print=636) allows to print the whole dataset, which had in this case 636 rows. The Coef and the VIP column contain the data that also the function uses for the plotting later. I prefer to use the plotted version of the data to delineate the phases, because this gives me a better overview over the longer development. options(max.print=636) plscf My estimated delineations are included in the table below. I wasnt able to delineate the chilling phase for the Chilling Hours model, because no model coefficients seemed to have an advancing effect. For both other models I was able to delineate the chilling period. For the GDH model, I set the delineation estimates once and did not change them, even though I observed slight changes in the advancing effects between the different chill models. Chilling phase Forcing phase Start End Start End Chilling Hours 0 0 8 106 Utah Model -13 65 8 106 Dynamic Model 24 62 8 106 20.3 Plot results for all three analyses, including shaded plot areas for the chilling and forcing periods you estimated. Below I plotted the estimated delineations together with the previously created function. For the Chilling Hours plot (as already mentioned above) no chilling period can be determined plot_PLS_chill_force(plscf, chill_metric=&quot;Chilling_Hours&quot;, heat_metric=&quot;GDH&quot;, chill_label=&quot;CH&quot;, heat_label=&quot;GDH&quot;, chill_phase=c(0,0), heat_phase=c(8,106), plot_title = &quot;Chilling Hours model&quot;) Figure 20.5: PLS regression plot for the response of the Chilling Hours model and GDH model for ‘Roter Boskoop’ apples at CKA plot_PLS_chill_force(plscf, chill_metric=&quot;Utah_Chill_Units&quot;, heat_metric=&quot;GDH&quot;, chill_label=&quot;CU&quot;, heat_label=&quot;GDH&quot;, chill_phase=c(-13,65), heat_phase=c(8,108), plot_title = &quot;Utah Chill model&quot;) Figure 20.6: PLS regression plot for the response of the Utah Chill model and GDH model for ‘Roter Boskoop’ apples at CKA plot_PLS_chill_force(plscf, chill_metric=&quot;Chill_Portions&quot;, heat_metric=&quot;GDH&quot;, chill_label=&quot;CP&quot;, heat_label=&quot;GDH&quot;, chill_phase=c(-24,62), heat_phase=c(8,108), plot_title = &quot;Dynamic Model&quot;) Figure 20.7: PLS regression plot for the response of the Dynamic Model and GDH model for ‘Roter Boskoop’ apples at CKA "],["examples-of-pls-regression-with-agroclimatic-metrics.html", "Chapter 21 Examples of PLS regression with agroclimatic metrics 21.1 Look across all the PLS results presented above. Can you detect a pattern in where chilling and forcing periods could be delineated clearly, and where this attempt failed? 21.2 Can you think about possible reasons for the success or failure of PLS analysis based on agroclimatic metrics (if so, write down your thoughts)?", " Chapter 21 Examples of PLS regression with agroclimatic metrics 21.1 Look across all the PLS results presented above. Can you detect a pattern in where chilling and forcing periods could be delineated clearly, and where this attempt failed? We can see that some of the PLS results can be easier delineated. The PLS analysis on Chinese data, showed no clear dormancy phases. There were some peaks, which could be assigned to the chilling or forcing period, but on the other side some contradictory coefficients were also seen, especially during the endodormancy. During the ecodormancy the results could be assigned easier and also in a more uniform way. Similar results can we also see in Germany, the UK and also Croatia. In countries like Tunisia or regions like California, we can see that the two phases can be recognized easier. The average temperature during winter and summer is lower in countries like China or Germany compared to Tunisia or California. Croatia is in between regarding there temperatures, but the PLS analysis do not show clear results. 21.2 Can you think about possible reasons for the success or failure of PLS analysis based on agroclimatic metrics (if so, write down your thoughts)? The PLS-analysis is based on variation in the data. This means that if our PLS analysis does not result in clear observations, one reason could be low variation in the data. In the next chapter, we can see how the data of the well performing countries is distributed in comparison to the chill model sensitivity. The data from these regions shows variation and is spread about areas of the functions, where chill is generated, but also on parts where chill is not generated, due to high temperatures. "],["why-pls-doesnt-always-work.html", "Chapter 22 Why PLS doesn’t always work 22.1 Produce chill and heat model sensitivity plots for the location you focused on in previous exercises. 22.2 Look at the plots for the agroclimate-based PLS analyses of the ‘Alexander Lucas’ and ‘Roter Boskoop’ datasets. Provide your best estimates of the chilling and forcing phases.", " Chapter 22 Why PLS doesn’t always work 22.1 Produce chill and heat model sensitivity plots for the location you focused on in previous exercises. The following loops create a data frame, that contains all the information to visualize the sensitivity of the Dynamic model to different temperature combinations. Since chilling is mainly relevant during dormancy, we calculate the model sensitivity only for the winter months. Cape Town, the place whose weather data I have already used for the previous tasks, is located in the southern hemisphere. This means that the summer and winter months are swapped with those of the northern hemisphere. coords &lt;- c(18.8148, -33.9575) latitude&lt;-coords[2] month_range=c(4,5,6,7,8,9) # South African winter Tmins=c(-20:20) Tmaxs=c(-15:30) mins&lt;-NA maxs&lt;-NA CP&lt;-NA month&lt;-NA temp_model&lt;-Dynamic_Model for(mon in month_range) {days_month&lt;-as.numeric(difftime( ISOdate(2002,mon+1,1), ISOdate(2002,mon,1) )) if(mon==12) days_month&lt;-31 weather&lt;-make_all_day_table(data.frame(Year=c(2001,2002), Month=c(mon,mon), Day=c(1,days_month),Tmin=c(0,0),Tmax=c(0,0))) for(tmin in Tmins) for(tmax in Tmaxs) if(tmax&gt;=tmin) { weather$Tmin&lt;-tmin weather$Tmax&lt;-tmax hourtemps&lt;-stack_hourly_temps(weather, latitude=latitude)$hourtemps$Temp CP&lt;-c(CP,do.call(Dynamic_Model, list(hourtemps))[length(hourtemps)]/(length(hourtemps)/24)) mins&lt;-c(mins,tmin) maxs&lt;-c(maxs,tmax) month&lt;-c(month,mon) } } results&lt;-data.frame(Month=month,Tmin=mins,Tmax=maxs,CP) results&lt;-results[!is.na(results$Month),] write.csv(results,&quot;data/model_sensitivity_development.csv&quot;,row.names = FALSE) library(ggplot2) library(colorRamps) results &lt;- read.csv(&quot;data/model_sensitivity_development.csv&quot;) results$Month_names&lt;- factor(results$Month, levels=month_range, labels=month.name[month_range]) DM_sensitivity&lt;-ggplot(results,aes(x=Tmin,y=Tmax,fill=CP)) + geom_tile() + scale_fill_gradientn(colours=alpha(matlab.like(15), alpha = .5), name=&quot;Chill/day (CP)&quot;) + ylim(min(results$Tmax),max(results$Tmax)) + ylim(min(results$Tmin),max(results$Tmin)) DM_sensitivity &lt;- DM_sensitivity + facet_wrap(vars(Month_names)) + ylim(min(results$Tmax),max(results$Tmax)) + ylim(min(results$Tmin),max(results$Tmin)) In order to link the plot with Cape Town, real weather recordings are included as points in the existing sensitivity plots. This visualizes how effective the recorded temperatures have been to generate chill. CT_temperatures&lt;-read_tab(&quot;data/CapeTown_weather.csv&quot;) CT_temperatures&lt;-CT_temperatures[which(CT_temperatures$Month %in% month_range),] CT_temperatures[which(CT_temperatures$Tmax&lt;CT_temperatures$Tmin), c(&quot;Tmax&quot;,&quot;Tmin&quot;)]&lt;-NA CT_temperatures$Month_names = factor(CT_temperatures$Month, levels=c(4,5,6,7,8,9), labels=c(&quot;April&quot;,&quot;May&quot;, &quot;June&quot;,&quot;July&quot;, &quot;August&quot;,&quot;September&quot;)) DM_sensitivity + geom_point(data=CT_temperatures,aes(x=Tmin,y=Tmax,fill=NULL, color=&quot;Temperature&quot;),size=0.2) + facet_wrap(vars(Month_names)) + scale_color_manual(values = &quot;black&quot;, labels = &quot;Daily temperature \\nextremes (°C)&quot;, name=&quot;Observed at site&quot; ) + guides(fill = guide_colorbar(order = 1), color = guide_legend(order = 2)) + ylab(&quot;Tmax (°C)&quot;) + xlab(&quot;Tmin (°C)&quot;) + theme_bw(base_size=15) Figure 22.1: Chill model sensitivity plot for Cape Town Compared to other regions where low temperatures result in enough chill, this is not the case in Cape Town. The plotted actual weather records in the plots overlap only to a small extent with the area in which chill is generated at all. Additionally, the generation of chill per day is significantly lower in the majority of the overlapping areas because the temperatures are too warm in general. The code to create the sensitivity data frames and the sensitivity plots is easier to handle within in functions. This also allows to import the functions at a later point in time. Again I hide the code for the functions to save some space. chill_model_sensitivity &lt;- function(latitude, temp_models=list(Dynamic_Model=Dynamic_Model,GDH=GDH), month_range=c(10,11,12,1,2,3), Tmins=c(-10:20), Tmaxs=c(-5:30)) { mins&lt;-NA maxs&lt;-NA metrics&lt;-as.list(rep(NA,length(temp_models))) names(metrics)&lt;-names(temp_models) month&lt;-NA for(mon in month_range) { days_month&lt;-as.numeric(difftime( ISOdate(2002,mon+1,1), ISOdate(2002,mon,1) )) if(mon==12) days_month&lt;-31 weather&lt;-make_all_day_table(data.frame(Year=c(2001,2002), Month=c(mon,mon), Day=c(1,days_month), Tmin=c(0,0),Tmax=c(0,0))) for(tmin in Tmins) for(tmax in Tmaxs) if(tmax&gt;=tmin) { weather$Tmin&lt;-tmin weather$Tmax&lt;-tmax hourtemps&lt;-stack_hourly_temps(weather, latitude=latitude)$hourtemps$Temp for(tm in 1:length(temp_models)) metrics[[tm]]&lt;-c(metrics[[tm]], do.call(temp_models[[tm]], list(hourtemps))[length(hourtemps)]/ (length(hourtemps)/24)) mins&lt;-c(mins,tmin) maxs&lt;-c(maxs,tmax) month&lt;-c(month,mon) } } results&lt;-cbind(data.frame(Month=month,Tmin=mins,Tmax=maxs), as.data.frame(metrics)) results&lt;-results[!is.na(results$Month),] } chill_sensitivity_temps &lt;- function(chill_model_sensitivity_table, temperatures, temp_model, month_range=c(10,11,12,1,2,3), Tmins=c(-10:20), Tmaxs=c(-5:30), legend_label=&quot;Chill/day (CP)&quot;) { library(ggplot2) library(colorRamps) cmst&lt;-chill_model_sensitivity_table cmst&lt;-cmst[which(cmst$Month %in% month_range),] cmst$Month_names&lt;- factor(cmst$Month, levels=month_range, labels=month.name[month_range]) DM_sensitivity&lt;- ggplot(cmst, aes_string(x=&quot;Tmin&quot;,y=&quot;Tmax&quot;,fill=temp_model)) + geom_tile() + scale_fill_gradientn(colours=alpha(matlab.like(15), alpha = .5), name=legend_label) + xlim(Tmins[1],Tmins[length(Tmins)]) + ylim(Tmaxs[1],Tmaxs[length(Tmaxs)]) temperatures&lt;- temperatures[which(temperatures$Month %in% month_range),] temperatures[which(temperatures$Tmax&lt;temperatures$Tmin), c(&quot;Tmax&quot;,&quot;Tmin&quot;)]&lt;-NA temperatures$Month_names &lt;- factor(temperatures$Month, levels=month_range, labels=month.name[month_range]) DM_sensitivity + geom_point(data=temperatures, aes(x=Tmin,y=Tmax,fill=NULL,color=&quot;Temperature&quot;), size=0.2) + facet_wrap(vars(Month_names)) + scale_color_manual(values = &quot;black&quot;, labels = &quot;Daily temperature \\nextremes (°C)&quot;, name=&quot;Observed at site&quot; ) + guides(fill = guide_colorbar(order = 1), color = guide_legend(order = 2)) + ylab(&quot;Tmax (°C)&quot;) + xlab(&quot;Tmin (°C)&quot;) + theme_bw(base_size=15) } Show/Hide code In the following code chunk, I create again a sensitivity data frame with the chill_model_sensitivity() function. This time I also add the Growing Degree Hours to the function. This allows me to use the chill_sensitivity_temps() function to create plots for chill and heat model sensitivity as seen below. model_sensitivities_CT &lt;- chill_model_sensitivity(latitude = coords[2], temp_models = list(Dynamic_Model=Dynamic_Model, GDH=GDH), month_range = c(4,5,6,7,8,9)) write.csv(model_sensitivities_CT, &quot;data/model_sensitivities_CT.csv&quot;,row.names = FALSE) model_sensitivities_CT &lt;- read.csv(&quot;data/model_sensitivities_CT.csv&quot;) chill_sensitivity_temps(model_sensitivities_CT, CT_temperatures, temp_model=&quot;Dynamic_Model&quot;, month_range=c(4,5,6,7,8,9), legend_label=&quot;Chill per day \\n(Dynamic Model)&quot;) + ggtitle(&quot;Chill model sensitivity at Cape Town, South Africa&quot;) Figure 22.2: Chill model sensitivity plot for Cape Town created with the function chill_sensitivity_temps(model_sensitivities_CT, CT_temperatures, temp_model=&quot;GDH&quot;, month_range=c(4,5,6,7,8,9), legend_label=&quot;Heat per day \\n(GDH)&quot;) + ggtitle(&quot;Heat model sensitivity at Cape Town, South Africa&quot;) Figure 22.3: Heat model sensitivity plot for Cape Town created with the function I already discussed the temperatures in Cape Town in the context of chill generation. I still plotted the sensitivity again, because the preset temperature ranges are more suitable with the warm temperatures of South Africa. The heat model sensitivity plot shows in which way the temperatures in Cape Town generate for the ecodormancy relevant heat. The amount of generated heat is sufficient and chill is the limiting factor for the phenology development. 22.2 Look at the plots for the agroclimate-based PLS analyses of the ‘Alexander Lucas’ and ‘Roter Boskoop’ datasets. Provide your best estimates of the chilling and forcing phases. temps&lt;-read_tab(&quot;data/TMaxTMin1958-2019_patched.csv&quot;) temps_hourly&lt;-stack_hourly_temps(temps,latitude=50.6) Boskoop &lt;- read_tab(&quot;data/Roter_Boskoop_bloom_1958_2019.csv&quot;) Boskoop_first &lt;- Boskoop[,1:2] Boskoop_first[,&quot;Year&quot;] &lt;- substr(Boskoop_first$First_bloom,1,4) Boskoop_first[,&quot;Month&quot;] &lt;- substr(Boskoop_first$First_bloom,5,6) Boskoop_first[,&quot;Day&quot;] &lt;- substr(Boskoop_first$First_bloom,7,8) Boskoop_first &lt;- make_JDay(Boskoop_first) Boskoop_first &lt;- Boskoop_first[,c(&quot;Pheno_year&quot;,&quot;JDay&quot;)] colnames(Boskoop_first) &lt;- c(&quot;Year&quot;,&quot;pheno&quot;) Alex &lt;- read_tab(&quot;data/Alexander_Lucas_bloom_1958_2019.csv&quot;) Alex_first &lt;- Alex[,1:2] Alex_first[,&quot;Year&quot;] &lt;- substr(Alex_first$First_bloom,1,4) Alex_first[,&quot;Month&quot;] &lt;- substr(Alex_first$First_bloom,5,6) Alex_first[,&quot;Day&quot;] &lt;- substr(Alex_first$First_bloom,7,8) Alex_first &lt;- make_JDay(Alex_first) Alex_first &lt;- Alex_first[,c(&quot;Pheno_year&quot;,&quot;JDay&quot;)] colnames(Alex_first) &lt;- c(&quot;Year&quot;,&quot;pheno&quot;) I added a title function to the plot_PLS_chill_force() function to add the cultivar name to the plots. The estimated delineations for the plots can be seen in the plots. The phases of the “Alexander Lucas” cultivar extend over a longer period of time, while the “Roter Boskoop” phases are shorter. The heat phase is easier to delineate compared to the chilling period, which can sometimes have a less significant beginning. source(&quot;functions/plot_PLS_chill_force.R&quot;) daychill &lt;- daily_chill(hourtemps=temps_hourly, running_mean=9, models = list(Chilling_Hours = Chilling_Hours, Utah_Chill_Units = Utah_Model, Chill_Portions = Dynamic_Model, GDH = GDH) ) plscf_alex &lt;- PLS_chill_force(daily_chill_obj=daychill, bio_data_frame=Alex_first, split_month=6, chill_models = c(&quot;Chill_Portions&quot;), heat_models = c(&quot;GDH&quot;)) plscf_boskoop &lt;- PLS_chill_force(daily_chill_obj=daychill, bio_data_frame=Boskoop_first, split_month=6, chill_models = c(&quot;Chill_Portions&quot;), heat_models = c(&quot;GDH&quot;)) plot_PLS_chill_force(plscf_alex, chill_metric=&quot;Chill_Portions&quot;, heat_metric=&quot;GDH&quot;, chill_label=&quot;CP&quot;, heat_label=&quot;GDH&quot;, chill_phase=c(-52,62), heat_phase=c(-5,106), plot_title = &quot;Alexander Lucas&quot;) Figure 22.4: Temperature response pattern of ‘Alexander Lucas’ apples at CKA based on a PLS regression. plot_PLS_chill_force(plscf_boskoop, chill_metric=&quot;Chill_Portions&quot;, heat_metric=&quot;GDH&quot;, chill_label=&quot;CP&quot;, heat_label=&quot;GDH&quot;, chill_phase=c(-24,62), heat_phase=c(10,108), plot_title = &quot;Roter Boskoop&quot;) Figure 22.5: Temperature response pattern of ‘Roter Boskoop’ apples at CKA based on a PLS regression. "],["evaluating-pls-outputs.html", "Chapter 23 Evaluating PLS outputs 23.1 Reproduce the analysis for the ‘Roter Boskoop’ dataset. 23.2 We’ve looked at data from a number of locations so far. How would you expect this surface plot to look like in Beijing? And how should it look in Tunisia?", " Chapter 23 Evaluating PLS outputs 23.1 Reproduce the analysis for the ‘Roter Boskoop’ dataset. In the first part, I try to delineate the chill and heat phases with the help of a PLS analysis. I load the phenology data and also the according temperatures. After that, I calculate the chill and the heat with the daily_chill() function. library(chillR) library(dplyr) library(purrr) Boskoop_first&lt;-read_tab(&quot;data/Roter_Boskoop_bloom_1958_2019.csv&quot;)[,1:2] %&gt;% mutate(Year=substr(First_bloom,1,4), Month=substr(First_bloom,5,6), Day=substr(First_bloom,7,8)) %&gt;% make_JDay() %&gt;% select(Pheno_year,JDay) %&gt;% rename(Year=1,pheno=2) temps&lt;-read_tab(&quot;data/TMaxTMin1958-2019_patched.csv&quot;) temps_hourly&lt;-read_tab(&quot;data/TMaxTMin1958-2019_patched.csv&quot;) %&gt;% stack_hourly_temps(latitude=50.6) daychill&lt;-daily_chill(hourtemps=temps_hourly, running_mean=1, models = list(Chilling_Hours = Chilling_Hours, Utah_Chill_Units = Utah_Model, Chill_Portions = Dynamic_Model, GDH = GDH) ) plscf&lt;-PLS_chill_force(daily_chill_obj=daychill, bio_data_frame=Boskoop_first, split_month=6, chill_models = &quot;Chill_Portions&quot;, heat_models = &quot;GDH&quot;, runn_means = 11) With the plot_PLS_chill_force() function, that we already created in one of the previous chapters, we can try to visualize the two phases. I adjusted the parameter for the beginning of the forcing phase to accurately delineate it. For the other delineations, the parameters from the website were suitable . source(&quot;functions/plot_PLS_chill_force.R&quot;) plot_PLS_chill_force(plscf, chill_metric=&quot;Chill_Portions&quot;, heat_metric=&quot;GDH&quot;, chill_label=&quot;CP&quot;, heat_label=&quot;GDH&quot;, chill_phase=c(-48,62), heat_phase=c(10,105.5)) Figure 23.1: PLS regression plot for the response of the Dynamic Model and GDH model for ‘Roter Boskoop’ apples at CKA The fields package contains tools to work with spatial data. In this case the krig() function was used to create an interpolated surface that shows the impact of the chilling and forcing temperatures on the bloom date. library(fields) library(reshape2) library(metR) library(ggplot2) library(colorRamps) pheno&lt;-Boskoop_first colnames(pheno)[1]&lt;-&quot;End_year&quot; chill_phase&lt;-c(317,62) heat_phase&lt;-c(10,105.5) mean_temp_period&lt;-function(temps, start_JDay, end_JDay, end_season = end_JDay) { temps_JDay&lt;-make_JDay(temps) temps_JDay[,&quot;Season&quot;]&lt;-temps_JDay$Year if(start_JDay&gt;end_season) temps_JDay$Season[which(temps_JDay$JDay&gt;=start_JDay)]&lt;- temps_JDay$Year[which(temps_JDay$JDay&gt;=start_JDay)]+1 if(start_JDay&gt;end_season) sub_temps&lt;-subset(temps_JDay,JDay&lt;=end_JDay|JDay&gt;=start_JDay) if(start_JDay&lt;=end_JDay) sub_temps&lt;-subset(temps_JDay,JDay&lt;=end_JDay&amp;JDay&gt;=start_JDay) mean_temps&lt;-aggregate(sub_temps[,c(&quot;Tmin&quot;,&quot;Tmax&quot;)], by=list(sub_temps$Season), FUN=function(x) mean(x, na.rm=TRUE)) mean_temps[,&quot;n_days&quot;]&lt;-aggregate(sub_temps[,&quot;Tmin&quot;], by=list(sub_temps$Season), FUN=length)[,2] mean_temps[,&quot;Tmean&quot;]&lt;-(mean_temps$Tmin+mean_temps$Tmax)/2 mean_temps&lt;-mean_temps[,c(1,4,2,3,5)] colnames(mean_temps)[1]&lt;-&quot;End_year&quot; return(mean_temps) } mean_temp_chill&lt;-mean_temp_period(temps = temps, start_JDay = chill_phase[1], end_JDay = chill_phase[2], end_season = heat_phase[2]) mean_temp_heat&lt;-mean_temp_period(temps = temps, start_JDay = heat_phase[1], end_JDay = heat_phase[2], end_season = heat_phase[2]) mean_temp_chill&lt;- mean_temp_chill[which(mean_temp_chill$n_days &gt;= max(mean_temp_chill$n_days)-1),] mean_temp_heat&lt;- mean_temp_heat[which(mean_temp_heat$n_days &gt;= max(mean_temp_heat$n_days)-1),] mean_chill&lt;-mean_temp_chill[,c(&quot;End_year&quot;,&quot;Tmean&quot;)] colnames(mean_chill)[2]&lt;-&quot;Tmean_chill&quot; mean_heat&lt;-mean_temp_heat[,c(&quot;End_year&quot;,&quot;Tmean&quot;)] colnames(mean_heat)[2]&lt;-&quot;Tmean_heat&quot; phase_Tmeans&lt;-merge(mean_chill,mean_heat, by=&quot;End_year&quot;) colnames(pheno)&lt;-c(&quot;End_year&quot;,&quot;pheno&quot;) Tmeans_pheno&lt;-merge(phase_Tmeans,pheno, by=&quot;End_year&quot;) # Kriging interpolation k &lt;- Krig(x=as.matrix(Tmeans_pheno[,c(&quot;Tmean_chill&quot;,&quot;Tmean_heat&quot;)]), Y=Tmeans_pheno$pheno, give.warnings = TRUE) pred &lt;- predictSurface(k) colnames(pred$z)&lt;-pred$y rownames(pred$z)&lt;-pred$x melted&lt;-melt(pred$z) colnames(melted)&lt;-c(&quot;Tmean_chill&quot;,&quot;Tmean_heat&quot;,&quot;value&quot;) N &lt;- pred$z[c(80:1),] rotate &lt;- function(x) t(apply(x, 2, rev)) B &lt;- rotate(N) For plotting the krigged matrix we can use the geom_contour_fill() function from ggplot2. This creates a two-dimensional surface where the bloom date is coded by color. One finding of the plot is that a warm forcing phase contributes significantly to earlier flowering. The black dots in the graph show the years that were used in the krigging process. Years with the most distinctive temperatures shape the outer border of the surface. I noticed that the selection of delineations has an impact on the generation of the graph. If the delineations from the “Alexander Lucas” dataset are used, a surface plot is created, which appears completely parallel and additionally the krig() function outputs a warning. In the two-dimensional plot I was already able to see it, but where it was visible even better is in the three-dimensional graph below. There the surface with the differently delineated phase parameters was a completely smooth plane, which did not fitted well to the given data. ggplot(melted, aes(x=Tmean_chill,y=Tmean_heat,z=value)) + geom_contour_fill(bins=100) + scale_fill_gradientn(colours=alpha(matlab.like(15)), name=&quot;Bloom date \\n(day of the year)&quot;) + geom_contour(col=&quot;black&quot;) + geom_point(data=Tmeans_pheno, aes(x=Tmean_chill,y=Tmean_heat,z=NULL), size=0.7) + geom_text_contour(stroke = 0.2) + ylab(expression(paste(&quot;Forcing phase &quot;, T[mean],&quot; (°C)&quot;))) + xlab(expression(paste(&quot;Chilling phase &quot;, T[mean],&quot; (°C)&quot;))) + theme_bw(base_size=15) Figure 23.2: Impact of chilling and forcing temperatures on “Roter Boskoop” Three-dimensional graphs rarely contain data that could not be extracted from two-dimensional graphs. The idea of actively interacting with the data fascinates me on the other hand. With plotly I came across a R package that offers this interactivity. In terms of the surface plot, I also liked the feature of being able to immediately see more detailed data from each individual point (interpolated as well as observed). This makes it possible, for example, to find out more about outliers like the point at the top right from the year 1963. Additionally, the influence of individual data points on the interpolation becomes more clearly visible. library(plotly) library(colorRamps) plot_ly() %&gt;% add_surface(x = pred$x, y = pred$y, z = B, colors = alpha(matlab.like(15)), name = &quot;Interpolated&quot;, hoverlabel = list(namelength = 0), hovertemplate = paste(&#39;&lt;/br&gt;&lt;b&gt;Interpolated&lt;/b&gt;&#39;, &#39;&lt;/br&gt;Chill T&lt;sub&gt;mean&lt;/sub&gt;: %{x:.2f}°C&#39;, &#39;&lt;/br&gt;Force T&lt;sub&gt;mean&lt;/sub&gt;: %{y:.2f}°C&#39;, &#39;&lt;/br&gt;Bloom date: %{z:.1f} JDay&#39;), contours = list( z = list(color = alpha(matlab.like(15)), highlight = TRUE, show = TRUE, start = 85, end = 150, size = 5, project=list(z=TRUE)), showlabels = TRUE)) %&gt;% add_trace(x = Tmeans_pheno$Tmean_chill, y = Tmeans_pheno$Tmean_heat, z = Tmeans_pheno$pheno, type = &#39;scatter3d&#39;, mode = &#39;markers&#39;, name = &quot;Observed&quot;, hoverinfo =&quot;text&quot;, text = ~paste(&#39;&lt;/br&gt;&lt;b&gt;Observed&lt;/b&gt;&#39;, &#39;&lt;/br&gt;Chill T&lt;sub&gt;mean&lt;/sub&gt;:&#39;, round(Tmeans_pheno$Tmean_chill, digits = 2),&#39;°C&#39;, &#39;&lt;/br&gt;Force T&lt;sub&gt;mean&lt;/sub&gt;:&#39;, round(Tmeans_pheno$Tmean_heat, digits = 2),&#39;°C&#39;, &#39;&lt;/br&gt;Bloom date:&#39;,Tmeans_pheno$pheno, &#39;JDay&#39;, &#39;&lt;/br&gt;Year:&#39;, as.character(Tmeans_pheno$End_year)), marker = list(size = 4, color = &#39;black&#39;)) %&gt;% colorbar(title = &quot;Bloom date\\n(day of the year)&quot;) %&gt;% layout( scene = list( xaxis = list(title = &quot;Chilling phase T&lt;sub&gt;mean&lt;/sub&gt; (°C)&quot;, showspikes=FALSE), yaxis = list(title = &quot;Forcing phase T&lt;sub&gt;mean&lt;/sub&gt; (°C)&quot;, showspikes=FALSE), zaxis = list(title = &quot;Date of Bloom&quot;, showspikes=FALSE) ))%&gt;% layout( scene = list( camera=list( #up = list(x=0, y=0, z=0), #center = list(x=0, y=0, z=0), eye = list(x=-1.2, y=2, z=1) ) ) ) Show/Hide code Figure 23.3: Three dimensional plot for the impact of chilling and forcing temperatures on “Roter Boskoop” 23.2 We’ve looked at data from a number of locations so far. How would you expect this surface plot to look like in Beijing? And how should it look in Tunisia? The angles of the contours of the plots show how the ratio of chilling and forcing influences the bloom date. For vertical contours, the chilling phase is the decisive factor, while for horizontal contours, the forcing phase mainly influences the bloom date. In the weather data of the CKA and the phenology data of the “Alexander Lucas” cultivar, both phases have a certain influence, but the forcing phase is still more dominant. This characteristic depends on the location and the given chilling and forcing conditions. In cold China heat is probably the determining factor while in Tunisia the phenological development is mainly limited by the chilling amount. "],["the-relative-importance-of-chill-and-heat.html", "Chapter 24 The relative importance of chill and heat 24.1 Describe the temperature response hypothesis outlined in this chapter.", " Chapter 24 The relative importance of chill and heat 24.1 Describe the temperature response hypothesis outlined in this chapter. The phenology temperature response hypothesis addresses how different climate in various regions affects the impact of the chilling and forcing period during the dormancy. In cold regions such as China or Germany, where chill is sufficient the temperatures of the forcing period determine the blossom development almost exclusively. The driver of the phenology development is the forcing phase. In warm regions such as Tunisia, where heat is abundant the temperatures in the chilling period determine the duration of dormancy. In these regions the driver for development is mainly the chilling. The cases where the length of dormancy is influenced by only almost one of the two phases are the most extreme cases. They represent the two ends of the spectrum of the temperature influence of the dormancy. In between there is a gradient in which both phases can have a substitutable influence. "],["making-valid-tree-phenology-models.html", "Chapter 25 Making valid tree phenology models 25.1 Explain the difference between output validation and process validation. 25.2 Explain what a validity domain is and why it is important to consider this whenever we want to use our model to forecast something. 25.3 What is validation for purpose? 25.4 How can we ensure that our model is suitable for the predictions we want to make?", " Chapter 25 Making valid tree phenology models 25.1 Explain the difference between output validation and process validation. The complicated processes in nature have often resulted in output validated models. In this case, only the model output is compared with real observations. The way in which this output has been generated is not further elucidated. Such output validated models are often based on machine learning methods like regression, neural networks or random forest approaches. The goal is to use the training data to create a model that gives a suitable output. With output validated models, good performance has little to do with an accurate model. Process validated models, on the other hand, aim to capture the real process behind the output. In order to be successful, certain mechanics must be known that contribute to the output result in nature. The models must therefore also be reviewed in the light of new research findings. In process validated models it is an important to model natural phenomena accurately without getting lost in too detailed approaches. In terms of phenology models, a process validated model should additionally be able to explain special events such as Saint Barbara cherry blossoms or other tree related flowering anecdotes. 25.2 Explain what a validity domain is and why it is important to consider this whenever we want to use our model to forecast something. The validity domain is the area of a model in which it can make reliable statements. Models can often make statements outside of their validity domain. However, it is important to recognize this and be cautious when necessary. The validity domain is often influenced by the model or the data it was calibrated on. If the calibration data are in a certain range, the validity domain covers to some extent only this range. This is especially important when making predictions about future events. Often the input data will be different in the future than it was in the past. Models trained only on past data have a validity domain that would not include future data. This leads to model results that are not credible. The model will still produce results, but since the validity domain has been exceeded, they are likely to be flawed. 25.3 What is validation for purpose? Validation for purpose considers whether a model is suitable for the actual application. Which features are part of the model and how exactly can a model fulfill its intention. If the model is validated for the wrong purpose, errors can occur during its later use. 25.4 How can we ensure that our model is suitable for the predictions we want to make? Regarding the validity domain, we need to check if the test data overlaps substantially with the data the model was calibrated on. If this is not the case, a possible option would be to increase the validity domain by adding artificially created data to the calibration. "],["the-phenoflex-model.html", "Chapter 26 The PhenoFlex model 26.1 Parameterize the PhenoFlex model for `Roter Boskoop’ apples. 26.2 Produce plots of predicted vs. observed bloom dates and distribution of prediction errors. 26.3 Compute the model performance metrics RMSEP, RPIQ, mean error and mean absolute error.", " Chapter 26 The PhenoFlex model 26.1 Parameterize the PhenoFlex model for `Roter Boskoop’ apples. The PhenoFlex model has 12 parameter that can be adjusted to the individual situation. In this example, we will parameterize the model for the “Roter Boskoop” cultivar. For succesful parameterizing, we need the actual time of Bloom of each year and the corresponding temperatures. library(chillR) CKA_weather&lt;-read_tab(&quot;data/TMaxTMin1958-2019_patched.csv&quot;) Boskoop&lt;-read_tab(&quot;data/Roter_Boskoop_bloom_1958_2019.csv&quot;) Boskoop_first&lt;-Boskoop[,1:2] Boskoop_first[,&quot;Year&quot;]&lt;-substr(Boskoop_first$First_bloom,1,4) Boskoop_first[,&quot;Month&quot;]&lt;-substr(Boskoop_first$First_bloom,5,6) Boskoop_first[,&quot;Day&quot;]&lt;-substr(Boskoop_first$First_bloom,7,8) Boskoop_first&lt;-make_JDay(Boskoop_first) Boskoop_first&lt;-Boskoop_first[,c(&quot;Pheno_year&quot;,&quot;JDay&quot;)] colnames(Boskoop_first)&lt;-c(&quot;Year&quot;,&quot;pheno&quot;) hourtemps &lt;- stack_hourly_temps(CKA_weather, latitude=50.6) For the parameters, we can set a starting point for each variable. Those starting points are already in use and are known to result in plausible predictions. We also specify a range in between which the algorithm searchs for suitable values. The broader the range, the longer the function needs to output its candidates. # yc, zc, s1, Tu, E0, E1, A0, A1, Tf, Tc, Tb, slope par &lt;- c(40, 190, 0.5, 25, 3372.8, 9900.3, 6319.5, 5.939917e13, 4, 36, 4, 1.60) upper &lt;- c(41, 200, 1.0, 30, 4000.0, 10000.0, 7000.0, 6.e13, 10, 40, 10, 50.00) lower &lt;- c(38, 180, 0.1, 0 , 3000.0, 9000.0, 6000.0, 5.e13, 0, 0, 0, 0.05) We use the phenologyFitter function for the fitting. The number of maximal iterations is set to 1000 to avoid ending in a local minimum, but finding the best values globally. SeasonList &lt;- genSeasonList(hourtemps$hourtemps, mrange = c(8, 6), years=c(1959:2018)) Fit_res &lt;- phenologyFitter(par.guess=par, modelfn = PhenoFlex_GDHwrapper, bloomJDays=Boskoop_first$pheno[which(Boskoop_first$Year&gt;1958)], SeasonList=SeasonList, lower=lower, upper=upper, control=list(smooth=FALSE, verbose=TRUE, maxit=1000, nb.stop.improvement=5)) Boskoop_par&lt;-Fit_res$par write.csv(Boskoop_par,&quot;data/PhenoFlex_parameters_Roter_Boskoop.csv&quot;) 26.2 Produce plots of predicted vs. observed bloom dates and distribution of prediction errors. For the previous task, we calculated the best fitting parameters, in the next code chunk, we use this list as input for the PhenoFlex_GDHwrapper function and store the results together with the observed data. For the plots we also calculate the difference between prediction and observation for each year. Boskoop_par&lt;-read_tab(&quot;data/PhenoFlex_parameters_Roter_Boskoop.csv&quot;)[,2] SeasonList &lt;- genSeasonList(hourtemps$hourtemps, mrange = c(8, 6), years=c(1959:2019)) Boskoop_PhenoFlex_predictions&lt;-Boskoop_first[which(Boskoop_first$Year&gt;1958),] for(y in 1:length(Boskoop_PhenoFlex_predictions$Year)) Boskoop_PhenoFlex_predictions$predicted[y]&lt;-PhenoFlex_GDHwrapper(SeasonList[[y]],Boskoop_par) Boskoop_PhenoFlex_predictions$Error&lt;- Boskoop_PhenoFlex_predictions$predicted-Boskoop_PhenoFlex_predictions$pheno library(ggplot2) ggplot(Boskoop_PhenoFlex_predictions,aes(x=pheno,y=predicted)) + geom_point() + geom_abline(intercept=0, slope=1) + theme_bw(base_size = 15) + xlab(&quot;Observed bloom date (Day of the year)&quot;) + ylab(&quot;Predicted bloom date (Day of the year)&quot;) + ggtitle(&quot;Predicted vs. observed bloom dates&quot;) Figure 26.1: A graph that shows the relation between the predicted and observed dates of bloom. The line represents a perfect model with no errors. ggplot(Boskoop_PhenoFlex_predictions,aes(Error)) + geom_histogram(binwidth = 2) + ggtitle(&quot;Distribution of prediction errors&quot;) Figure 26.2: A histogram that shows distribution of the errors. We can see that the models predicts the blooming dates more often too early. 26.3 Compute the model performance metrics RMSEP, RPIQ, mean error and mean absolute error. With the following commands from the chillR package and from base R, we can calculate standard performance metrics. boskoop_RMSEP &lt;- RMSEP(Boskoop_PhenoFlex_predictions$predicted,Boskoop_PhenoFlex_predictions$pheno) boskoop_RPIQ &lt;- RPIQ(Boskoop_PhenoFlex_predictions$predicted,Boskoop_PhenoFlex_predictions$pheno) boskoop_mean &lt;- mean(Boskoop_PhenoFlex_predictions$Error) boskoop_absolute_error &lt;- mean(abs(Boskoop_PhenoFlex_predictions$Error)) A RMSEP of 3.87 and a RPIQ of 3.68 are good results for the performance of our model. Compared to the RMSEP, the RPIQ not only includes the error, but also the observed variation. The mean error of -1.08 shows that the model tends to predict the and the absolute error of 2.86 "],["the-phenoflex-model---a-second-look.html", "Chapter 27 The PhenoFlex model - a second look 27.1 Make chill and heat response plots for the ‘Alexander Lucas’ PhenoFlex model for the location you did the earlier analyses for.", " Chapter 27 The PhenoFlex model - a second look 27.1 Make chill and heat response plots for the ‘Alexander Lucas’ PhenoFlex model for the location you did the earlier analyses for. First we need to set some parameters for the generation of the different temperatures. In the case of South Africa, we set the latitude to 33.95° South. Because of the location in the southern hemisphere it is also important to change the range of the month from April to September. require(chillR) require(lubridate) PhenoFlex_parameters_Alex &lt;- read_tab(&quot;data/PhenoFlex_parameters_Alexander_Lucas.csv&quot;)[,2] coords &lt;- c(18.8148, -33.9575) latitude&lt;-coords[2] #month_range&lt;-c(10,11,12,1,2,3) month_range=c(4,5,6,7,8,9) Tmins=c(-20:20) Tmaxs=c(-15:30) mins&lt;-NA maxs&lt;-NA chill_eff&lt;-NA heat_eff&lt;-NA month&lt;-NA With the parameters set, we can calculate the model sensitivity to our generated temperature dataframe. The result is saved to be imported later. for(mon in month_range) { weather&lt;-make_all_day_table(data.frame(Year=c(2001,2001), Month=c(mon,mon), Day=c(1,days_in_month(mon)), Tmin=c(0,0), Tmax=c(0,0))) for(tmin in Tmins) for(tmax in Tmaxs) if(tmax&gt;=tmin) { weather$Tmin&lt;-tmin weather$Tmax&lt;-tmax hourtemps&lt;-stack_hourly_temps(weather, latitude=latitude)$hourtemps$Temp chill_eff&lt;-c(chill_eff, tail(PhenoFlex(temp=hourtemps, times=c(1: length(hourtemps)), A0=PhenoFlex_parameters_Alex[7], A1=PhenoFlex_parameters_Alex[8], E0=PhenoFlex_parameters_Alex[5], E1=PhenoFlex_parameters_Alex[6], Tf=PhenoFlex_parameters_Alex[9], slope=PhenoFlex_parameters_Alex[12], deg_celsius=TRUE, basic_output=FALSE)$y,1)/ days_in_month(mon)) heat_eff&lt;-c(heat_eff, tail(cumsum(GDH_response(hourtemps, PhenoFlex_parameters_Alex)),1)/ days_in_month(mon)) mins&lt;-c(mins,tmin) maxs&lt;-c(maxs,tmax) month&lt;-c(month,mon) } } results&lt;-data.frame(Month=month,Tmin=mins,Tmax=maxs,Chill_eff=chill_eff,Heat_eff=heat_eff) results&lt;-results[!is.na(results$Month),] write.csv(results,&quot;data/model_sensitivity_PhenoFlex.csv&quot;) For the chill and heat response plots, we use the functions created in the earlier chapter, where we already conducted similar analysis. require(ggplot2) require(colorRamps) require(tidyr) require(reshape2) source(&quot;functions/chill_sensitivity_temps.R&quot;) model_sens_PhenoFlex &lt;- read.csv(&quot;data/model_sensitivity_PhenoFlex.csv&quot;) CapeTown_weather &lt;- read_tab(&quot;data/CapeTown_chillR_weather.csv&quot;) We can see that the calibrated PhenoFlex parameters of the “Alexander Lucas” cultivar have almost no overlap regarding the chill sensitivity with the observed temperatures at Cape Town. Fulfilling the heat requirements however would not be a problem. chill_sensitivity_temps(model_sens_PhenoFlex, CapeTown_weather, temp_model=&quot;Chill_eff&quot;, month_range=c(4,5,6,7,8,9), Tmins=c(-20:20), Tmaxs=c(-15:30), legend_label=&quot;Chill per day \\n(arbitrary)&quot;) + ggtitle(&quot;PhenoFlex chill sensitivity (&#39;Alexander Lucas&#39;)&quot;) Figure 27.1: PhenoFlex chill sensitivity for the “Alexander Lucas” parameters chill_sensitivity_temps(model_sens_PhenoFlex, CapeTown_weather, temp_model=&quot;Heat_eff&quot;, month_range=c(4,5,6,7,8,9), Tmins=c(-20:20), Tmaxs=c(-15:30), legend_label=&quot;Heat per day \\n(arbitrary)&quot;) + ggtitle(&quot;PhenoFlex heat sensitivity (&#39;Alexander Lucas&#39;)&quot;) Figure 27.2: PhenoFlex heat sensitivity for the “Alexander Lucas” parameters "],["can-we-improve-the-performance-of-phenoflex.html", "Chapter 28 Can we improve the performance of PhenoFlex? 28.1 What was the objective of this work? 28.2 What was the main conclusion? 28.3 What experiments could we conduct to test the hypothesis that emerged at the end of the conclusion?", " Chapter 28 Can we improve the performance of PhenoFlex? 28.1 What was the objective of this work? The objective of this work was to apply data from earlier collected experimental seasons to the PhenoFlex model. The main part of this was to calibrate the PhenoFlex model with normal phenology data and with data that additionally contained also 5 marginal phenology records. This helped to show the impact of the marginal seasons on the performance of the PhenoFlex model. 28.2 What was the main conclusion? The main conclusion is that the model whose calibration also contained the marginal data has a lower performance compared to the calibration without the warm seasons. One possible hypothesis for the decline in model performance is that the bloom in the marginal season occurred through alternative mechanisms compared to the regular chill force process. These alternative mechanisms are not considered by the model. Therefore data outside the validity range can negatively affect the model parameters. 28.3 What experiments could we conduct to test the hypothesis that emerged at the end of the conclusion? Flowering can be triggered not only by sufficient chill during the chilling phase and heat during the forcing phase, but also by other partly stress-related mechanisms. Producers in growing regions with insufficient chill for example use chemicals that induce stress which eventually can also lead to flowering in trees. In an experiment plant hormones or gene regulation connected to stress could be measured at trees under different climate conditions. Under marginal conditions such stress levels could be increased, which would support the hypothesis that the blossom in marginal seasons is induced by stress for example. "],["frost-risk-analysis.html", "Chapter 29 Frost risk analysis 29.1 Download the phenology dataset for the apple cultivar Roter Boskoop from Klein-Altendorf. 29.2 Illustrate the development of the bloom period over the duration of the weather record. Use multiple ways to show this - feel free to be creative. 29.3 Evaluate the occurrence of frost events at Klein-Altendorf since 1958. Illustrate this in a plot. 29.4 Produce an illustration of the relationship between spring frost events and the bloom period of ‘Roter Boskoop.’ 29.5 Evaluate how the risk of spring frost for this cultivar has changed over time. Has there been a significant trend?", " Chapter 29 Frost risk analysis 29.1 Download the phenology dataset for the apple cultivar Roter Boskoop from Klein-Altendorf. I loaded the files directly from the data directory. I then used the melt() function to shape the data frame for the use with ggplot2. CKA_Roter_Boskoop&lt;-read.csv(&quot;data/Roter_Boskoop_bloom_1958_2019.csv&quot;) CKA_weather&lt;-read.csv(&quot;data/TMaxTMin1958-2019_patched.csv&quot;) library(reshape2) library(chillR) Roter_Boskoop &lt;- melt(CKA_Roter_Boskoop, id.vars = &quot;Pheno_year&quot;, value.name=&quot;YEARMODA&quot;) Roter_Boskoop$Year &lt;- as.numeric(substr(Roter_Boskoop$YEARMODA,1,4)) Roter_Boskoop$Month &lt;- as.numeric(substr(Roter_Boskoop$YEARMODA,5,6)) Roter_Boskoop$Day &lt;- as.numeric(substr(Roter_Boskoop$YEARMODA,7,8)) Roter_Boskoop &lt;- make_JDay(Roter_Boskoop) 29.2 Illustrate the development of the bloom period over the duration of the weather record. Use multiple ways to show this - feel free to be creative. I tried different plot types to visualize the development over the bloom period. To see the development, the year must be represented in the plot in some way. The bar chart give a good overview and also show that the bloom occurs increasingly earlier in the year. I used the facet_wrap() function to split the plot also in its three subparts to also visualize the advance within the flowering. library(ggplot2) library(ggExtra) library(tidyverse) bloom_sections &lt;- list( &quot;First_bloom&quot; = &quot;First bloom&quot;, &quot;Full_bloom&quot; = &quot;Full bloom&quot;, &quot;Last_bloom&quot; = &quot;Last bloom&quot; ) bloom_labeller &lt;- function(variable,value){ return(bloom_sections[value])} Roter_Boskoop &lt;- Roter_Boskoop[order(nrow(Roter_Boskoop):1),] RB_phenology &lt;- ggplot(Roter_Boskoop, aes(x = Pheno_year, y = JDay, fill = variable)) + geom_bar(stat = &quot;identity&quot;) + scale_fill_discrete(name = &quot;Bloom sections&quot;, labels=c(&#39;First bloom&#39;, &#39;Full bloom&#39;, &#39;Last bloom&#39;)) + theme_bw() + theme(axis.title.x = element_text(size = 20), axis.title.y = element_text(size = 20), legend.title = element_text(size = 20), legend.text = element_text(size = 20)) RB_phenology Figure 29.1: Flower development bar chart RB_phenology_split &lt;- RB_phenology + facet_wrap(Roter_Boskoop$variable,labeller = bloom_labeller) + theme_bw() + theme(axis.title.x = element_text(size = 20), axis.title.y = element_text(size = 20), legend.title = element_text(size = 20), legend.text = element_text(size = 20), strip.text.x = element_text(size = 20)) RB_phenology_split Figure 29.2: Splitted flower development bar chart The animated point plot shows in a different way how the beginning of flowering moves forward. The color of the dots represents the years, resulting in darker dots for the early years and in lighter dots for the recent years. While the darker dots tend to accumulate at a later time, the lighter dots tend to be earlier. library(gganimate) PG_phenology &lt;- ggplot(Roter_Boskoop, aes(x=JDay, y=variable, fill=Year, color=Year)) + geom_point(aes(group=Year, size=20)) + transition_reveal(along=Year) + facet_wrap(Roter_Boskoop$variable, labeller = bloom_labeller) + theme_bw() + theme(axis.title.y=element_blank(), axis.title.x = element_text(size = 20), axis.text.y=element_blank(), axis.ticks.y=element_blank(), legend.title = element_text(size = 20), legend.text = element_text(size = 20), strip.text.x = element_text(size = 20)) + guides(size = FALSE) PG_phenology Figure 29.3: Animated point plot for the flower development This animated graph is based on a line graph that shows the development of the three different phenology dates. To plot this graph, I applied the runn_mean() function on the data. This reduces the number of extreme values and makes the plot easier to view. Roter_Boskoop[,&quot;runn_mean_JDay&quot;] &lt;- runn_mean(Roter_Boskoop$JDay,4) LP_Phenology &lt;- ggplot(Roter_Boskoop, aes(x=Year, y=runn_mean_JDay, group=variable, color=variable)) + geom_point(size=4) + geom_line(size=1.5) + scale_color_discrete( name=&quot;Bloom sections&quot;, labels=c(&quot;First bloom&quot;, &quot;Full bloom&quot;, &quot;Last bloom&quot;)) + xlab(&quot;Season (Year)&quot;) + ylab(&quot;Julian date&quot;) + theme_bw() + theme(axis.title.x = element_text(size = 20), axis.title.y = element_text(size = 20), legend.title = element_text(size = 20), legend.text = element_text(size = 20)) + guides(size = FALSE) + transition_reveal(Year) animate(LP_Phenology, height = 600, width =1200) Figure 29.4: Animated line plot for the flower development 29.3 Evaluate the occurrence of frost events at Klein-Altendorf since 1958. Illustrate this in a plot. The graphs illustrates the development of yearly frost hours since 1958. It can be seen that there is a trend towards fewer frost hours. Also the geom_smooth() line shows this trend. To plot this data, we need a function that counts the frost hours in the data. The frost hours are automatically summarized for every year, which simplifies plotting. frost_df=data.frame( lower=c(-1000,0), upper=c(0,1000), weight=c(1,0)) frost_model&lt;-function(x) step_model(x,frost_df) hourly&lt;-stack_hourly_temps(CKA_weather,latitude=50.625) frost&lt;-tempResponse(hourly,models=c(frost=frost_model)) ggplot(frost, aes(End_year,frost)) + geom_bar(stat = &quot;identity&quot;) + geom_smooth(color=&quot;red&quot;) + ylim(c(0,NA)) + ylab(&quot;Frost hours per year&quot;) + xlab(&quot;Year&quot;) + theme_bw() + theme(axis.title.x = element_text(size = 20), axis.title.y = element_text(size = 20), legend.title = element_text(size = 20), legend.text = element_text(size = 20)) Figure 29.5: Development of yearly frost hours since 1958 with a trend line 29.4 Produce an illustration of the relationship between spring frost events and the bloom period of ‘Roter Boskoop.’ For the next graph, we use the daily amount of frost hours. This can be done with the aggregate() function, so that all days with frost events can be plotted in the next step. frost_model_no_summ&lt;-function(x) step_model(x, frost_df, summ=FALSE) hourly$hourtemps[,&quot;frost&quot;]&lt;-frost_model_no_summ(hourly$hourtemps$Temp) Daily_frost_hours&lt;-aggregate(hourly$hourtemps$frost, by=list(hourly$hourtemps$YEARMODA), FUN=sum) Daily_frost&lt;-make_JDay(CKA_weather) Daily_frost[,&quot;Frost_hours&quot;]&lt;-Daily_frost_hours$x Ribbon_Boskoop&lt;-dcast( Roter_Boskoop,Pheno_year ~ variable, value.var = &quot;JDay&quot;) lookup_dates&lt;-Ribbon_Boskoop row.names(lookup_dates)&lt;-lookup_dates$Pheno_year Daily_frost[,&quot;First_bloom&quot;]&lt;- lookup_dates[as.character(Daily_frost$Year),&quot;First_bloom&quot;] Daily_frost[,&quot;Last_bloom&quot;]&lt;- lookup_dates[as.character(Daily_frost$Year),&quot;Last_bloom&quot;] Daily_frost[ which(!is.na(Daily_frost$Frost_hours)),&quot;Bloom_frost&quot;]&lt;- &quot;Before bloom&quot; Daily_frost[ which(Daily_frost$JDay&gt;=Daily_frost$First_bloom),&quot;Bloom_frost&quot;]&lt;- &quot;During bloom&quot; Daily_frost[ which(Daily_frost$JDay&gt;Daily_frost$Last_bloom),&quot;Bloom_frost&quot;]&lt;- &quot;After bloom&quot; Daily_frost[ which(Daily_frost$JDay&gt;180),&quot;Bloom_frost&quot;]&lt;- &quot;Before bloom&quot; ggplot(data=Ribbon_Boskoop,aes(Pheno_year)) + geom_ribbon(aes(ymin = First_bloom, ymax = Last_bloom), fill = &quot;light gray&quot;) + geom_line(aes(y = Full_bloom)) + theme_bw(base_size=15) + xlab(&quot;Phenological year&quot;) + ylab(&quot;Julian date (day of the year)&quot;) + geom_point(data=Daily_frost, aes(Year,JDay,size=Frost_hours,col=Bloom_frost), alpha = 0.8) + scale_size(range = c(0, 5), breaks = c(1, 5, 10, 15, 20), labels = c(&quot;1&quot;, &quot;5&quot;, &quot;10&quot;, &quot;15&quot;, &quot;20&quot;), name=&quot;Frost hours&quot;) + scale_color_manual( breaks=c(&quot;Before bloom&quot;, &quot;During bloom&quot;, &quot;After bloom&quot;), values=c(&quot;light green&quot;,&quot;red&quot;,&quot;light blue&quot;), name=&quot;Frost timing&quot;) + theme_bw(base_size=15) + ylim(c(80,140)) Figure 29.6: Timing of frost depending on the day of the year and the tree phenology 29.5 Evaluate how the risk of spring frost for this cultivar has changed over time. Has there been a significant trend? Bloom_frost_trend&lt;-aggregate( Daily_frost$Frost_hours, by=list(Daily_frost$Year,Daily_frost$Bloom_frost), FUN=function(x) sum(x,na.rm=TRUE)) colnames(Bloom_frost_trend)&lt;-c(&quot;Year&quot;,&quot;Frost_timing&quot;,&quot;Frost_hours&quot;) DuringBloom&lt;- Bloom_frost_trend[which(Bloom_frost_trend$Frost_timing==&quot;During bloom&quot;),] ggplot(data=DuringBloom,aes(Year,Frost_hours)) + geom_col() + geom_smooth() Figure 29.7: Relevant frost hours with geom_smooth() trend require(Kendall) Kendall(x = DuringBloom$Year,y = DuringBloom$Frost_hours) ## tau = 0.0333, 2-sided pvalue =0.74038 Spring frosts are one of the biggest threats to total fruit loss for apple growers. Therefore, any frost during the flowering period should be taken seriously. The graph shows that in the past and also in the most recent years there were always dangerous frost periods. A significant trend cannot be determined with the Kendall test, nevertheless the value tau of 0.03 rather indicates an increase. If we consider that the beginning of flowering is pushed forward by climate change, we can expect more frost during the flowering period in the future. Growers should get early information on regional circumstances and possibilities for frost damage prevention. "]]
